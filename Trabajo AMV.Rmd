---
title: "MARKETING CAMPAIGN"
output:
  html_document:
    code_folding: show
    keep_md: true
    css: styles.css
    toc: true
    toc_float: true
    number_sections: true
runtime: shiny

---
<style type="text/css">

  theme{
      bg: "#ffffff" 
      fg: "#010101"
      progress-bar-bg: "orange"
      primary: "#add8e6"
      secondary: "#ffffff"}
      
</style>

<div style="text-align:center;font-weight:bold">
<div/>

```{r setup, include=FALSE}
#LIBRERIAS
library(gganimate)
library(fpp3)
library(hopkins)
library(gower)
library(cluster)
library(tidyverse)
library(hrbrthemes)
library(viridis)
library(patchwork)
library(ggplot2)
library(ggExtra)
library(DT)
library(shiny)
library(ggpubr)
library(lazyeval) 
library(e1071)
library(RColorBrewer)
library(ggpubr)
library(plotly)
library(kableExtra)
library(summarytools)
library(Matrix)
library(GGally)
library(ggcorrplot)
library("FactoMineR")
library("factoextra")
library(corrplot)
library(caret)
library(plotly)
library(fpc)
library(dbscan)
library(fmsb)
library(gplots)
library(psych)
library(paletteer)
```

<div style="text-align: justify;font-weight:normal"> 
<br />

```{r echo = FALSE}
paletaContinua <- c("#BFEFFF", "#FFC0CB","#FFF68F", "#d2f7d2", "#607B8B", "#AB82FF", "#A9A9A9", "#FFEC8B" , "#97FFFF", "##FF6A6A", "#836FFF", "#698B22", "#B23AEE", "#00BFFF", "#0000FF", "#9F79EE")
```


# INTRODUCCIÓN 

Este documento desarrollará un enfoque de minería de datos para predecir el éxito de las llamadas de telemarketing cuyo objetivo es vender depósitos bancarios a largo plazo. Para la recogida de datos, se abordó un banco minorista portugués, con datos seleccionados desde 2008 hasta 2013, incluyendo así los efectos de la reciente crisis financiera. Se analizó un amplio conjunto de 150 características relacionadas con el cliente bancario, el producto y los atributos socioeconómicos. En la fase de modelización se hizo especial hincapié en la ingeniería de características y se logró reducir el conjunto de variables inicial a uno reducido de 21 atributos. 

Las campañas de venta de marketing constituyen una estrategia típica para potenciar el negocio. Las empresas utilizan el marketing directo cuando se dirigen a segmentos de clientes poniéndose en contacto con ellos para alcanzar un objetivo específico. La centralización de las interacciones remotas con los clientes en un centro de contacto facilita la gestión operativa de las campañas. Estos centros permiten comunicarse con los clientes a través de varios canales, siendo el teléfono (fijo o móvil) uno de los más utilizados. Hay que destacar que la tarea de seleccionar el mejor conjunto de clientes, es decir, los que tienen más probabilidades de suscribir un producto, se considera un problema NP-hard. 

Como consecuencia, resulta evidente que en el sector bancario, la optimización de la segmentación del telemarketing es una cuestión clave, bajo una creciente presión para aumentar los beneficios y reducir los costes. En este contexto, el uso de un sistema de apoyo a la toma de decisiones basado en un modelo impulsado por un análisis de datos es fundamental para predecir el resultado de una llamada telefónica de telemarketing.

En consiguiente, en el documento se tratara de modelar el éxito de suscripción que tienen los clientes ante las campañas de marketing llevadas a cabo por la entidad bancaria, se hará uso de varias técnicas estadísticas con las que se crearán perfiles de clientes que tengan mayor probabilidad de suscripción o se generará información relevante para futuras campañas.

<div/>
<br />

## CONJUNTO DE DATOS Y VARIABLES

Para comenzar con el trabajo, se explicará detalladamente el conjunto de datos. Para ello, se tiene una pequeña muestra de las 5 primeras observaciones del dataset total.

```{r echo=FALSE}
datos <- read.csv("C:/Users/unaig/Desktop/IKASKETAK/UNIBERTSITATEA/3.URTEA/2.TARTEA/AMV/TRABAJO/Grupo10_MarketingCampaign/MarketingCampaign/bank-additional-full.csv", sep = ";")

datos1<- datos
datos %>% head(5) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  column_spec(column = 21, width = "3cm", background = "#ecfefe") %>%
  scroll_box(width = "900px", height = "250")
```
<br />

```{r}
print(paste('Número de ejemplos / instancias totales del dataset: ', nrow(datos))) 
print(paste('Número de variables totales del dataset: ', ncol(datos)))
```

<div style="text-align: justify"> 

El dataset sobre que el se va a trabajar está compuesto por 41.188 ejemplos (filas) y 20 atributos (columnas), además de una adicional que indica la clase con la que se clasifica cada ejemplo. En esta ocasión, se trata de un problema de clasificación binaria, en la que la variable respuesta puede tomar dos valores: *"no"* o *"yes"*. 

También se puede hacer un primer resumen de entrada, que a grandes rasgos resulta útil para comprender el tipo de datos con los que se va a trabajar, el rango de valores que abarcan y sus mediciones estadísticas más comunes. Todos estos aspectos se irán explicando en profundidad a lo largo del escrito.


<div/>

```{r echo=FALSE}
summary(datos) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  column_spec(column = 22, width = "3cm", background = "#ecfefe") %>%
  scroll_box(width = "900px", height = "450")
```

<div style="text-align: justify"> 
<br />

## PROBLEMA NO BALANCEADO

En los problemas de clasificación en numerosas ocasiones surge el inconveniente de que en el conjunto de datos alguna de las clases de muestra es una clase “minoritaria”. Es decir, de la cual se tienen muy pocas muestras. Esto provoca un desbalanceo, dando lugar a un problema no balanceado, característica que habrá que considerar a lo largo del estudio y se tratará de razonar. 

Por lo general, este hecho afecta a los algoritmos en su proceso de generalización de la información, perjudicando a las clases minoritarias. Supongamos que queremos predecir la venta de depósitos bancarios a largo plazo. Como el número de llamadas exitosas es mucho menor que el número de llamadas fallidas, parece lógico que el modelo predeciría todas las llamadas como fallidas. Si programamos un modelo estadístico maximizando la accuracy (porcentaje de aciertos totales), el modelo podría predecir todas las las llamadas como no exitosas (no se consigue vender el depósito). Se obtendría un accuracy muy elevado, pero se estarían pasando por alto las llamadas que realmente si que obtendrían "yes" como respuesta.

La razón de este desbalanceo de clases es evidente; hay muchos más clientes que rechazan la compra del depósito frente a los que la aceptan. Sin embargo, esta clase minoritaria es la que realmente le interesa a la campaña de marketing, por lo que resulta de elevada importancia hallar las características, similitudes y diferencias que presenta este segmento de mercado reducido. 

En la siguiente tabla se pueden observar los individuos que hay por cada categoría ("no" o "yes"), el porcentaje que estos suponen sobre la muestra total, los valores faltantes de cada clase y algunos aspectos más de interés.

<div/>

#### Tabla de Frecuencias

***

```{r echo=FALSE, warning=FALSE}

tabla <- freq(datos$y, style = "simple", justify = "center", headings = TRUE) %>%
  kbl(caption = "") %>%
  kableExtra::kable_styling(full_width = F)

tabla
```

Podemos observar que la variable de salida cuyo valor es "no" equivale al 88.73458% de las observaciones totales, mientras que "yes" el 11.26542%. Para poder analizar gráficamente el desbalanceo entre categorías se puede generar también un histograma junto a un gráfico de puntos que permite detectar visualmente la diferencia entre el número de individuos de las dos clases.

```{css, echo=FALSE}
/*Centrar gráficas plotly*/
.center {
  display: table;
  margin-right: auto;
  margin-left: auto;
}
```
<div class = 'center'>
```{r fig.align="center",echo=FALSE, warning=FALSE}

myPalette <- brewer.pal(12, "Set3") 

counts<-data.frame(datos%>%group_by(y)%>%count())
colnames(counts)=c("y","Counts")
  
hist<-ggplot(counts, aes(x=y, y=Counts)) +
    geom_bar(position="dodge", stat="identity",aes(fill = y), alpha = 0.8)+
    coord_flip() +
    scale_fill_manual(values = paletaContinua) +
    theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour="#FFFFFF"))+xlab("y")

ggplotly(hist)
```
</div>
<br />

# ANÁLISIS UNIVARIANTE
<br />
A continuación, se tratará cada variable individualmente donde se analizará la existencia de outliers, la variabilidad, la homogeneidad y la simetría de estas a través de varios estadísticos y gráficas.

<br />

## DEFINICIÓN DE LAS VARIABLES

```{r echo = FALSE}
numerics<-datos%>%select_if(is.numeric)  
factors<-datos%>%select_if(negate(is.numeric))
```

<br />
**Variables Categóricas**

- *job*: trabajo que realiza el contacto (variable categórica que puede tomar los siguientes valores: *admin.*, *blue-collar*, *entrepreneur*, *housemaid*, *management*, *retired*, *self_employed*, *serivces*, *student*, *technician*, *unemployed*, *unknown*).

- *marital*: indica el estado civil del contaco (puede tomar las siguientes categorías: *divorced*, *married*, *single*, *unknown*).

- *default*: indica si el cliente contactado tiene impagos, es decir, si tiene créditos pendientes (es una variable categórica que indica si la cuestión es afirmativa, *yes* equivale a que el contacto tiene impagos, *no* el contacto no tiene impagos y *unknow* no se tiene esa información).

- *education*: tipo de educación recibida por el contacto (variable categórica con las siguientes categorías: *basic.4y*, *basic.6y*, *basci.9y*, *high.school*, *illiterate*, *professional.course*, *university.degree*, *unknown*).

- *housing*: indica si el contacto tiene un préstamo para la vivienda (es una variable categórica que indica si la cuestión es afirmativa, *yes*, *no* , *unknow* ).

- *loan*: indica si el contacto tiene un préstamos personal (es una variable categórica que indica si la cuestión es afirmativa, *yes*, *no* , *unknow* ).

- *contact*: tipo de comunicación del contacto (es una variable binaria que puede tomar dos valores: *cellular*, *telephone*).

- *month*: indica el mes del último contacto del año (es una variable categórica multiclase que toma un valor dentro del rango de los 12 meses del año: *jan*, *feb*, *mar*, ..., *nov*, *dec*).

- *day_of_week*: indica el día del último contacto de la semana (es una variable categórica multiclase que toma un valor dentro del rango de los 7 días de la semana: *mon*, *tue*, *wed*, *thu*, *fri*).

- *poutcome*: refleja el resultado de la campaña de marketing anterior (es una variable categórica que puede tomar tres valores: *failure*, *unknown*, *success*).

- *y*: es la variable de salida y el objeto del análisis de datos. Es una variable categórica que indica si el cliente ha suscrito un depósito a largo plazo o no (toma uno de los siguientes valores: *yes*, *no*).

<br />
**Variables Numéricas**

- *age*: edad del cliente contactado.

- *duration*: duración del último contacto en segundos (es una variable numérica). Es muy importante tener en cuenta que este atributo afecta en gran medida al valor de la variable de salida (por ejemplo, si duration=0 entonces y='no'). Sin embargo, la duración no se conoce antes de realizar una llamada. Además, después de la finalización de la llamada es obvio que se conoce. Por lo tanto, esta entrada sólo debería incluirse a efectos de referencia y debería descartarse si la intención es tener un modelo de predicción realista.

- *campaign*: número de contactos realizados durante esta campaña y para este cliente (variable numérica que incluye los últimos contactos).

- *pdays*: número de días transcurridos desde que se contactó por última vez con el cliente desde una campaña anterior (es una variable numérica. El valor 999 significa que no se contactó previamente con el cliente).

- *previous*: número de contactos realizados antes de esta campaña y para este cliente.

- *cons.price.idx*: índice mensual de precios al consumidor.

- *cons.conf.idx*: índice de confianza del consumidor (indicador mensual).

- *euribor3m*: tasa euribor a 3 meses (indicador diario).

- *nr.employed*: número de empleados (indicador trimestral).

<br />

### VALORES FALTANTES (NA)

Si se analiza la cantidad de valores perdidos de todo el dataset, se observa que no existen valores de variables NA. Es decir, no hay valores perdidos. Este hecho resulta realmente ventajoso y es un reflejo de que se ha realizado un buen estudio previo de las variables, la información a recoger y de que se ha realizado un esfuerzo en lograr un conjunto de datos completo y consistente. 

```{r}
print(paste("Número de datos faltantes en todo el dataset: ", sum(is.na(datos))))
```

Cabe mencionar que en varias variables categóricas del conjunto de datos existe en una categoría *unknown*, la cual se refiere que el usuario contactado no ha proporcionado esa información. Puede parecer que se trata de un valor perdido, no obstante, el hecho de que el usuario no proporcione esta información puede causar el éxito o el fracaso de la llamada. En consiguiente, no modificamos ni eliminamos estos datos ya que consideramos que nos pueden aportar información relevante.

<br />

## VALORES ANÓMALOS - OUTLIERS

Un valor atípico (outlier) es una observación que es significativamente distante del resto de los datos, esto es, observaciones con características diferentes de las demás. Las estadísticas derivadas de los conjuntos de datos que incluyen valores atípicos pueden estar potencialmente distorsionadas. Por tanto, es de gran relevancia detectar los valores anómalos de un conjunto de datos y saber cómo tratar con ellos; no se deben ignorar ni suprimir directamente, sino comprender el por qué de dicho valor y saber tratar con ello, es decir, se debe evaluar el tipo de información que pueden proporcionar.

Hay 4 principales tipos de outliers:

- Casos atípicos que surgen de un error de procedimiento.
- Observaciones que ocurren como consecuencia de un acontecimiento extraordinario.
- Observaciones cuyos valores caen dentro del rango de las variables observadas pero que son especiales por la combinación de valores.
- Datos extraordinarios para las que el investigador no tiene explicación.

Una forma fácil de detectar outliers es mediante gráficos. Una posibilidad sería realizar un diagrama de caja y bigotes, en el que se sitúa una caja, cuyo borde superior es Q3 y el inferior es Q1. Entre medias están el 50% de las ocurrencias. La altura de la caja es el rango intercuartílico, y el bigote, es la mediana. Por encima y debajo se ven dos límites, que son los umbrales para los valores atípicos. Como extensión del diagrama de caja es el del violín. Un diagrama de violín se utiliza para visualizar la distribución de los datos y su densidad de probabilidad, lo que permite detectar fácilmente valores extraños. Por otra parte, también es importante conocer la función de distribución de los datos para poder analizar los valores que incurren en mayor densidad de masa y analizar las posibles colas con menor número de individuos contabilizados. Todo ello permitirá, dada una variable específica, una visión global del conjunto de datos, de tal forma que el análisis y detección de observaciones anómalas será visualmente sencilla. Además, de este modo se conocerá también la distribución de los datos, lo que proporcionará información valiosa para el estudio.

<br />

## Estudio de las variables {.tabset .tabset-pills}

### Estudio para las variables categóricas 

En este apartado se realizará un análisis de las variables categóricas mediante un gráfico de sectores donde se tratará la frecuencia de las categorías de la variable a analizar y un gráfico de barras donde se estudiará la frecuencia de las categorías en función del éxito de la llamada.

<br />

```{r echo=FALSE}
inputPanel(
  shiny::selectInput("intro",label="Selección de variables", choices = factors%>%select(-y)%>%colnames())
)
```

```{r echo=FALSE, warning=FALSE}
renderPlot({
  library(RColorBrewer)
  myPalette <- brewer.pal(12, "Set3") 

  counts<-data.frame(factors%>%group_by_(input$intro)%>%count())
  colnames(counts)=c("var","Counts")
  
  p1<-ggplot(counts, aes(x="", y=Counts, fill=var)) +
      geom_bar(stat="identity", width=1) +
      coord_polar("y", start=0) +
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  
  counts<-data.frame(factors%>%group_by_(input$intro)%>%count(y))
  colnames(counts)=c("var","y","Counts")
  
  p2<-ggplot(counts, aes(x=var,y=Counts, fill=y)) +
      geom_bar(position="dodge", stat="identity")+
      geom_text(aes(label=Counts), position=position_dodge(width=0.9), vjust=-0.25,angle = 270,size=3)+
      coord_flip() +
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))+xlab(input$intro)
  
 ((p1 + theme(plot.margin = unit(c(0,60,0,0), "pt")))+  p2)
  
}, height=7*75)
```

```{r echo=FALSE}
renderUI({
  if(input$intro=="job"){
    HTML(paste(
    'La variable <i>job</i> indica los niveles de estudio que tiene el usuario contactado. Esta variable puede tomar 12 posibles valores, los cuales se han comentado anteriormente. Gracias al gráfico de sector y de barras estudiaremos la distribución de los tipos de empleos en los datos y en función de la variable salida.<br /><br />   
    
    Comenzando por el gráfico de sectores, se aprecia que los administrativos forman un cuarto de la base de datos, así como los obreros. Por otro lado, se observa una gran presencia de empleos en los sectores técnicos y de servicios. También, se observa que entre los contactados la cantidad de usuarios sin empleo son una minoría así como los autónomos, los jubilados y los estudiantes.<br /><br />   
    
    Si se analiza la distribución de la variable en función de la respuesta recibida a través de la gráfica de barras, observamos que entre los empleos más frecuentes se rechaza con mucha mayor frecuencia. Sin embargo, es interesante que los perfiles de estudiantes y retirados aceptan con mayor frecuencia en proporción a la cantidad de usuarios de este tipo, lo que puede indicar que la campaña tenía una mayor efectividad entre jóvenes estudiantes o clientes retirados.'))
  }
  else if(input$intro=="marital"){
    HTML(paste('
    La variable <i>marital</i> indica el estado civil del cliente con el que se ha establecido la comunicación donde los posibles valores a tomar son si está casado, divorciado o soltero (también existe la posibilidad de que el usuario no haya proporcionado esta información). 
    <br /><br />   
    En el gráfico de sectores podemos observar que más de la mitad de los usuarios contactados se encuentran casados en el momento de la llamada, más de un cuarto se encuentra sin pareja y los usuarios divorciados representan una cantidad menor a la de un cuarto de los datos.
    <br /><br />   
    Por otro lado, con el gráfico de barras, se observa que hay mayor cantidad de clientes casados que se han suscrito que en el resto de categorías, sin embargo, este aumento se debe a que la cantidad de llamadas a este tipo de perfil es mayor que en el resto de características. Por ello, puede llegar a ser más relevante estudiar porque los individuos solteros y divorciados aceptan en mayor proporción que los individuos casados.
               '))
  }
  else if(input$intro=="default"){
    
    dYes<-datos%>%filter(default=="yes")%>%count()
    dYesNo<-datos%>%filter(default=="yes",y=="no")%>%count()#==3
    dYesYes<-datos%>%filter(default=="yes",y=="yes")%>%count()#==0
    
    HTML(paste('La variable <i>default</i> indica si el individuo tiene o no algún impago.<br /><br />   
               
               Claramente se observa que la mayoría de los usuarios contactados no tienen impagos según la información que se posee. Por otro lado, se observa que la campaña no se interesó en perfiles de usuario que tuviesen impagos dado que solo se realizaron ',dYes, 'llamadas y ninguna de esas llamadas fue exitosa (por ello en las gráficas visualizadas no se observa los datos correspondientes a yes ya que no tienen relevancia con el resto de categorías).<br /><br /> 
               
               En consecuencia, al tener un número tan reducido de la característica <i>yes</i>, dada la gestión llevada a cabo en la campaña, creemos que este factor no sera de gran relevancia para lograr nuestro objetivo, debido a que no hay información suficiente de la individuos con impagos en la base de datos. 
               
               '))
  }
  else if(input$intro=="education"){
    HTML(paste('La variable <i>education</i> recoge el nivel de estudios de los individuos contactados.<br /><br /> 
               
               Se observa una representación relevante de cada categoría, aunque es evidente que la campaña se centró en individuos con perfiles universitarios. Luego estos son los que mayor proporción de éxito tienen, dada su mayor tamaño de muestra dentro del conjunto de datos. Por otro lado, individuos con cursos profesionales pueden llegar a ofrecer una mayor frecuencia de éxito en proporción al número de llamadas que se ha hecho a este perfil, por lo que sería interesante el estudio de estos.<br /><br />
               
               No obstante las diferencias entre distintas categorías a la hora de tener una llamda exitosa, se intuye que se debe a la diferencia de número de llamadas realizadas para cada tipo de educación y no a alguna relación relevante.'))
  }
  else if(input$intro=="housing"){
    HTML(paste('La variable <i>housing</i> indica si el individuo con el que se ha contactado tiene o no un préstamo de vivienda.<br /><br />
               
               Gracias al gráfico de sectores se puede observar que hay más individuos que tienen este tipo de préstamo que individuos que no lo tienen, no obstante, la proporción de ambas categorías son casi idénticas.<br /><br />
               
               Luego, en la gráfica de barras podemos observar que hay una mayor proporción de éxito entre los individuos que tienen un préstamo de este tipo en activo, sin embargo, la diferencia entre ambas categorías no es muy significativa por lo que la disparidad puede ser causada por la leve diferencia de tamaños muestrales de cada caso.<br /><br /> 
               
               En consiguiente, se puede intuir que esta variable no será de gran interés en el estudio a realizar.'))
  }
  else if(input$intro=="loan"){
    HTML(paste('La variable <i>loan</i> indica si el usuario contactado tiene o no un préstamo personal.<br /><br />
               
               Primeramente, en el gráfico de sectores se puede observar que la mayor parte de los datos pertenecen a individuos que no tienen este tipo de préstamo (llegando a ser más de tres cuartas partes del dataset).<br /><br /> 
               
               Por otro lado, el segundo gráfico, nos informa de que hay una mayor proporción de éxitos entre los que no tienen este préstamo, pero esto es a causa de la diferencia de tamaños que hay entre una característica y otra. En consecuencia, probablemente esta variable no proporcione información relevante.'))
  }
  else if(input$intro=="contact"){
    HTML(paste('La variable <i>contact</i> es categórica e indica el tipo de comunicación a través del cual se hizo el contacto. Es una variable binaria que puede tomar dos valores: “cellular” o “teléfono”. A través del gráfico de sectores y de barras podremos analizar la distribución de cada una de estas clases, y en especial, estudiarla en función de la variable de salida.
    
    <br/><br/>
               
               En primer lugar, a través del primer gráfico la conclusión que se obtiene es directa; un tercio de contactos se hacen a través del teléfono fijo, mientras que dos tercios a través del teléfono móvil. El segundo gráfico indica la distribución de estas clases en función de la variable de salida. Quizás este estudio sea más interesante, ya que puede arrojar más información de cara a análisis posteriores. La clase "cellular" son los teléfonos móviles, mientras que "telephone" los teléfonos fijos. Se observa que en ambas categorías predomina la variable de salida "no", es decir, hay muchos más clientes que replican con una respuesta negativa que con una positiva. Por otra parte, destaca también un mayor número de respuestas positivas cuando la vía de contacto es el teléfono móvil (cellular), que el teléfono fijo. Esto podría resultar realmente interesante de cara a favorecer una vía de contacto más que otra.'))
  }
  
  else if(input$intro=="month"){
    HTML(paste('La variable <i>month</i> indica el mes del último contacto del año, es decir, se trata de una variable categórica con 12 posibles categorías, correspondientes cada una de ellas a los 12 meses del año. <br /><br />
               
               A través del gráfico de sectores se puede observar una notable variabilidad por meses. Mientras que mayo abarca prácticamente 1/3 de los contactos totales, septiembre, octubre, marzo, abril y diciembre presentan muy pocos contactos, que de hecho, entre todos ellos no suman ni la mitad de las llamadas realizadas en mayo. Julio, junio, agosto y noviembre se encuentran en un intermedio entre los meses previamente mencionados con un número de contactos notable. En cuanto al diagrama de barras, podemos analizar el comportamiento en función del mes del año y en especial, según el valor de la variable de salida. Esto es, en función de si se consiguió vender o no al cliente el depósito bancario a largo plazo. En el eje de ordenadas se contabilizan el número de individuos, mientras que en el eje de abscisas los meses del año. Desde un primer momento se aprecia que predomina el color de la barra azul, lo que reitera lo que ya se sabía; hay muchos más clientes a los que no se les consigue vender el depósito de los que sí. Sin embargo, hay una situación curiosa y es que hay meses durante los cuales las llamadas telefónicas parecen disminuir. Si se analizan los meses de diciembre, marzo, octubre y septiembre, se aprecia cómo el número de individuos contactados es muy inferior al resto de periodos del año. De hecho, el número de clientes que responden "no" y los que responden "sí" es prácticamente igual. Los meses de abril y noviemmbre también parecen tener un menor número de contactos, seguidos de agosto, julio y junio. En estos últimos ya repuntan el número de individuos que responden "no" frente a los que finalmente compran los depósitos, hasta destacar finalmente en el mes de mayo, que son aparentemente los días del año donde más hincapié se hace en esta propaganda telefónica. A través de este gráfico de barras se observa que a medida que aumentan los contactos con clientes, aumentan también los individuos que responden que "no". No obstante, parece mantenerse constante los individuos que finalmente compran el depósito. No parece un hecho estacional el responder que "sí" a la propaganda, y aunque en mayo se puedan contabilizar más clientes convencidos, se debe más bien al aumento del número de contactos que al mes del año en el que se efectúan las llamadas. '))    
  }
  else if(input$intro=="day_of_week"){
    HTML(paste('La variable <i>days_of_week</i> es categórica e indica el día del último contacto de la semana. Es una variable categórica multiclase que toma un valor dentro del rango de los 7 días de la semana.     <br /><br />
    
    A través del diagrama de sectores se observa una proporción muy similar en cada día de la semana, característica que también veremos en el diagrama de barras. Asimismo, con el diagrama de barras se aprecia una tendencia constante durante la semana. Prácticamente todos los días de la semana se realiza el mismo número de contactos, con respuestas muy similares en todos. Generalmente, el número de últimos contactos diarios se sitúa en torno a 8.000 o 9.000 llamadas, de las cuales 7.000/8.000 reciben un "no" por respuesta, mientras que aproximadamente 1.000 clientes acaban comprando los depósitos del banco. Realmente se aprecia una tendencia muy similar durante los 5 días, viéndose quizás ligeramente incrementada los lunes y jueves con un mayor número de últimos contactos. Desde un primer análisis descriptivo y univariante, no parece que la variable "days_of_week" ofrezca mucha información y sea muy reveladora para el estudio bancario.'))  
  }
  else if(input$intro=="poutcome"){
    HTML(paste('
    La variable <i>poutcome</i> refleja la calidad del resultado de la campaña de marketing anterior. Esta variable categórica puede tomar los valores "failure", "nonexistent" o "success".
    <br /><br />   
    En el gráfico de sectores podemos observar que el resultado de la anterior campaña de marketing para más de tres cuartos de los usuarios contactados es inexistente. Asimismo, la proporción de fracasos en los resultados es mucho mayor que la proporción de éxitos.
    <br /><br />   
    Por otro lado, con el gráfico de barras se observa un mismo comportamiento cuando el resultado de la campaña es inexistente o de fracaso. En estos casos percibimos un mayor número de clientes que no suscriben un depósito a largo plazo frente a los que sí que lo suscriben. Resaltamos también que, como cabía esperar, en caso obtener un resultado exitoso, el número de clientes que suscriben un préstamo a largo plazo es mayor que el número de clientes que no lo suscriben. La variable proporciona información acorde a nuestra intuición.
               '))
  }
})
```

<br />

### Estudio para las variables numéricas

En este apartado se estudiarán las variables numéricas del dataset, donde a través de gráficas podremos tratar la distribución de valores en la variable. Resultara de gran interés hacerlo también en función de la variable de salida, puesto que el estudio de marketing está realmente focalizado en comprender cuándo se acepta o rechaza el depósito bancario. Asimismo, se utilizarán también los coeficientes de asimetría, el estadístico de *Kurtois* y la media con la mediana. 

Ahora, se explicará brevemente la información que proporciona cada estadístico.

**Coeficiente de asimería**

Este coeficiente mide la simetría de los datos respecto a su centro. Cuando el coeficiente es cercano a cero se concluye que la variable tiene una distribución simétrica, en el caso de que el coeficiente sea aproximadamente mayor que uno se puede concluir que los datos tienen una distribución asimétrica.

**Estadístico de Kurtois**

Este estadístico mide la homogeneidad de la variable, es decir medimos si hay o no datos que se separan mucho de la media (la existencia de este tipo de datos se denomina heterogeneidad).

Cuando el estadístico tiene un valor alto se puede intuir la existencia de outliers, en el caso de que su valor sea menor a dos también se observa heterogeneidad debido a que tenemos una mezcla de dos poblaciones de manera que una proporción importante de los datos, entre el 25% y el 50%, son heterogéneos con el resto.


**Mediana y media. Centralidad**

Se calculan estas dos medidas dado que cuando ambas son similares, la media es un buen indicador del centro de los datos. No obstante, si difieren mucho, la media podría no ser una buena medida del centro a causa de diversos factores (distribución asimétrica, heterogeneidad de los datos o presencia de outliers). 

<br/>
```{r echo=FALSE}
inputPanel(
  shiny::selectInput("variable", label="Selección de variables", choices = numerics%>%colnames())
)
```


```{r echo=FALSE}
renderPlot({
  datosDF<-data.frame(var=pull(datos1%>%select(input$variable)),y=pull(datos1%>%select(y)))
  
   p1 <- ggplot(data=datosDF, aes(x="", y=var)) + 
      geom_violin(color="black", fill="#a5bfde") +
      xlab("class") +
      theme(legend.position="none") +
      xlab("")+ylab(input$variable) + 
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p2 <- ggplot(data=datosDF, aes(x=var, fill=y)) +
      geom_density(alpha=0.3)+xlab(input$variable) + 
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p <-  ggplot(datosDF, aes(x=var, y=y)) +
        scale_fill_brewer(palette="Set3")+
        geom_point() +
        geom_line(color='blue')+
        theme(legend.position="none")+xlab(input$variable) +
        theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p3 <- ggMarginal(p, margins = 'x', color="blue", size=10)
  
        

  (p1 | p3) / p2
  
}, height=7*75)
```

```{r echo=FALSE}
renderUI({
  data<-pull(datos1%>%select(input$variable))
  
  varianza<-var(data)
  media<-mean(data)
  n<-length(data)
  curtosis<-(sum((data-media)^4)/varianza^2)/n
  asimetria<-skewness(data, na.rm = TRUE, type = 3)
  
  mediana<-median(data)
  HTML(paste("<br/>Kurtosis ",curtosis,"<br/>Coeficiente de asimetría:",asimetria,"<br/>Media:",
             media,", Mediana: ", mediana))
})
```
<br />
```{r echo=FALSE}
renderUI({
  if(input$variable=="age"){
    HTML(paste('
    La variable <i>age</i> es numérica y recoge las edades de los clientes contactados. Es una variable con una distribución muy amplia, donde tenemos edades entre 17 y 98 años. La gran variabilidad del factor se puede observar en la primera gráfica, donde observamos que hay un mayor contacto con usuarios comprendidos entre los 25 y 50 años.
    
    <br/><br/>

Por otro lado, la segunda gráfica no nos aporta gran información dada la amplia distribución de la variable y la gran cantidad de datos del dataset. En cambio, en la tercera gráfica se puede observar que en los extremos del intervalo de valores se obtiene un mayor número de respuestas positivas, sin embargo, se puede razonar con la menor cantidad de datos que encontramos en esas edades. Además, en el rango aproximado de 30 a 60 años, el intervalo más contactado, se observa que hay un mayor número de respuestas negativas que positivas. En consiguiente, se intuye que las campañas realizadas tienen una mayor efectividad entre perfiles jóvenes o de elevada edad.

    <br/><br/>
        
    Finalmente, podemos observar que la variable <i>age</i> no se podría llegar a considerar total asimétrica dado que su coeficiente de asimetría no es mayor que uno, sin embargo, gracias a las gráficas observamos que tampoco es simétrica aunque no se intuye que esa leve asimetría pueda perjudicar. Por otro lado, el estadístico de kurtosis es menor a dos por lo que podría darse el caso de tener valores atípicos que provoquen heterogeneidad, no obstante, somos conscientes de que hay individuos con edades muy alejadas en la base datos dada la amplitud de la distribución y no a medidas erróneas de los datos, por ello, consideramos que estos valores sean outliers. Por último, observamos que la media y la mediana son similares por lo que podríamos decir que la variable está correctamente centrada.'))
  }
  else if(input$variable=="duration"){
    HTML(paste('La variable <i>duration</i> recoge datos en segundos del último contacto con el cliente. Es importante destacar que esta variable es realmente significativa ya que afecta en gran medida a la variable de salida. Claramente, si no ha habido contacto con el cliente (0 segundos), no se habrá logrado convencerle de nada, puesto que es imposible. Esto conlleva a que, siempre que duration sea igual a 0, entonces la variable de salida adoptará el valor "no". <br /><br />   
               
               En este caso, mediante el primer gráfico se puede observar cómo la mayoría de llamadas duran pocos segundos. Existe una concentración de valores bajos, acentuada por el hecho de que, al tratarse de llamadas publicitarias, las personas tienden a colgar rápido. A través del segundo gráfico, independientemente de cuál sea la respuesta, se aprecia que gran parte de las llamadas se sitúan en el rango de 0 a 500 segundos, es decir, no llegan a 10 minutos. En general, se tienen llamadas cortas, en especial cuando la respuesta final es negativa. A través del tercer y último gráfico se observa cómo a medida que las llamadas son más largas, la probabilidad de obtener una respuesta final positiva aumenta. Las llamadas que finalmente obtienen como valor de salida "no" duran en general muy poco tiempo y se distribuyen en torno a 50 segundos, con una cola derecha más larga pero de menor masa. Claramente se observa que la distribución de aquellas llamadas que reciben un "sí" de respuesta es muy diferente. Son llamadas más largas, con una distribución en torno a los 5 minutos y con una cola mucho más densa a la derecha, indicador de lo previamente mencionado: a medida que transcurren los segundos de la llamada, la probabilidad de convicción es mayor. <br /><br />   
               
               Contextualizada la variable duration, se da por supuesto que en general es habitual obtener valores bajos para este atributo Sin embargo, esto no indica que un valor más elevado sea anómalo, simplemente la llamada podría haber durado más tiempo. No obstante, a través del primer y segundo gráfico se aprecian quizás dos valores excesivamente grandes: en torno a 4250 y casi 5000 segundos, lo que sería aproximadamente 1 hora y 20 minutos. En el primer caso, quizás sea correcto, ya que la respuesta obtenida es positiva y existe la posibilidad de que la llamada se alargue debido a explicaciones eventuales. En cuanto al segundo valor, convendría estudiarlo a través de la medida estadística de kurtosis para descartar un posible valor anómalo que distorsione el estudio, ya que, además, se obtiene como valor de salida "no", cuando generalmente estas llamadas son significativamente más cortas.  <br /><br /> 
               
               Finalmente, se observa que la variable <i>duration</i> es totalmente asimétrica ya que su coeficiente de asimetría es muy superior a uno. De hecho, toma el valor de 3.2629035825107. Al obtener un valor muy superior a la unidad, y positivo, diremos que existe mayor concentración de valores a la derecha de la media que a su izquierda, lo que también veíamos en el gráfico de densidades con una larga cola a la derecha. Por otro lado, el estadístico de kurtosis es muy superior a dos, adquiriendo el valor 23.2442. Claramente se trata de un valor excesivamente elevado, que resuelve completamente las dudas de la existencia de outliers: los datos no están próximos a la media y existe una gran heterogeneidad de los mismos. No hay duda de que las observaciones previamente mencionadas son anómalas y habrá que suprimirlas. Por último, observamos que la media y la mediana tampoco son similares por lo que la variable no está correctamente centrada.'))    
  }
  
  else if(input$variable=="campaign"){
    HTML(paste('La variable <i>campaign</i> es numérica y recoge las veces que se ha contactado con un cliente en una campaña dada. De los tres gráficos se obtiene una conclusión muy similar, se trata de un atributo muy variable, que presenta distribuciones con grandes oscilaciones y valores de  diversas características. En general, se aprecia una fluctuación en los valores de este atributo, con subidas y bajadas marcadas que muestran cierta inestabilidad en los valores observados.  <br /><br />         
               
               En un primer lugar, a través del primer gráfico se puede observar cómo generalmente el número de contactos con los clientes es bajo. La gran mayoría de datos se sitúa en un rango de 1 a 10 llamadas, con una masa más marcada en valores inferiores. Hay una apreciación curiosa y es que, aparentemente hay un número de llamadas que se evita. Es decir, hay valores que parecen obviados o eludidos, ya que con muy pocos clientes se contacta ese número de veces. Sin embargo, esto se debe a que esta variable, a pesar de ser numérica, únicamente puede tomar valores enteros. Es decir, las oscilaciones se deben a que no es posible contactar un número no entero de veces con un cliente. Esto explica las fluctuaciones que se aprecian en los gráficos y dan una respuesta sencilla a las mismas; campaign es una variable numérica entera. Del segundo gráfico concluimos que el número de veces que se contacta con cada cliente es bajo y se sitúa entre 1 y 5 mayoritariamente, siendo los valores pequeños los más comunes. Además, parece incluso que se consigue persuadir al cliente en las primeras llamadas. Es decir, aparentemente la insistencia no conlleva a que el cliente responda que "sí". La variable de salida "no" tiene un número de contactos mayor que cuando se recibe una respuesta positiva, lo que puede ser un indicador de lo previamente mencionado. Finalmente, en el tercer gráfico se observa que para valores de salida "sí" y "no" las distribuciones son muy similares. Destaca la cola derecha que es de mayor densidad cuando la variable de salida es "no", pero aparentemente la diferencia no es muy significativa. <br /><br />
               
               Habiendo contextualizado la variable campaign, se observa que es habitual obtener valores bajos para este atributo. En general, la mayoría de valores se sitúa en menos de 10 contactos, lo que hace sospechar que quizás sí que haya valores outliers. Existe una observación cuyo valor para la variable campaign es superior a 50 contactos. Esto podría ser cierto, pero hay que estudiar este caso aislado y ver si realmente se acepta o distorsiona considerablemente el estudio. Además, existen otras observaciones en torno a 40 contactos que también son dudosas. Convendría aplicar la medida estadística adecuda para asumir o descartar la anomalía de estos individuos. Realmente, prácticamente cualquier valor superior a 20 contactos es extraño, luego se tratará de estudiar todos ellos y analizar su viabilidad. <br /><br />
               
               Finalmente, se observa que la variable <i>campaign</i> es totalmente asimétrica, de hecho, lo es más aún que duration, ya que su coeficiente de asimetría es muy superior a uno, tomando el valor de 4.76215981717883. Al obtener un valor muy superior a la unidad, y positivo, diremos, al igual que antes, que existe mayor concentración de valores a la derecha de la media que a su izquierda. Por otro lado, el estadístico de kurtosis es muy superior a dos, adquiriendo el valor 39.9732193952114. Sin lugar a dudas se trata de un valor excesivamente elevado, luego los datos no están próximos a la media y existe una gran heterogeneidad de los mismos. Por tanto, las observaciones previamente mencionadas son anómalas y habrá que suprimirlas, en especial aquellas que sobrepasan la veintena de contactos por cliente, que desde luego, son excesivamente elevadas. Por último, observamos que la media y la mediana son similares por lo que la variable sí está correctamente centrada. '))    
  }
  else if(input$variable=="pdays"){
    HTML(paste('La variable <i>pdays</i> es numérica y recoge el número de días transcurridos desde que se contactó por última vez con el cliente desde una campaña anterior. Si se tiene el valor 999, significa que no se contactó con dicho cliente en ocasiones previas. <br /><br />
    
               En un primer lugar, a través del primer gráfico se puede observar cómo prácticamente todas las observaciones se sitúan en el rango del valor 999, es decir, con casi todos los clientes no se había contactado en ocasiones anteriores. Se analiza también que existen ciertas observaciones con valores muy bajos, indicando que con dichos individuos sí se había contactado previamente, pero recientemente. Del segundo gráfico concluimos lo mismo, se observa que la distribución se sitúa en torno al valor 999 con una larga cola a la derecha. Este gráfico nos reafirma en lo que ya sabíamos; con casi todos los clientes no se había contactado en campañas anteriores, mientras que hay unos pocos con los que sí, pero en ocasiones recientes. De hecho, no existen valores intermedios, o se tienen contactos muy recientes o directamente no se había contactado antes. Finalmente, en el tercer gráfico podemos distinguir las distribuciones en función del valor de la variable de salida. Se observa que para valores negativos ("no"), prácticamente nunca se había contactado antes. Por el contrario, cuando la respuesta es "sí" se aprecia que en más ocasiones había habido contactos previos. Esto es un indicador de que la insistencia puede alterar la decisión final del cliente. Se aprecia que la distribución para la variable de salida "sí" es mucho más plana y con mucha menor masa en torno al valor 999. Además, se ve cómo la densidad crece en los primeros valores de la variable. <br /><br />
               
               Contextualizada la variable pdays, se observa que es habitual obtener valores altos para este atributo, de hecho, el valor más repetido es 999. Sin embargo, existen también valores bajos, que aunque sean menos comunes, no son extraños. Esto se debe a que es completamente normal haber contactado con un cliente en campañas previas. Aparentemente no existen valores anómalos que exijan un estudio específico de la observación, ya que no hay valores excesivamente elevados que no sean 999 y el resto se distribuyen en un rango común y habitual para esta variable en concreto. <br /><br /> 
               
               Para concluir, se aprecia que la variable <i>pdays</i> vuelve a ser totalmente asimétrica ya que su coeficiente de asimetría es muy inferior a menos uno. De hecho, toma el valor de -4.92183140591174 . Al obtener un valor muy inferior a la unidad, y negativo, diremos que existe mayor concentración de valores a la izquierda de la media que a su derecha, lo que también veíamos en el gráfico de densidades con una larga cola a la izquierda. Sin embargo, este estadístico está claramente distorsionado por el valor 999, indicador de que no se contactó con el cliente en ocasiones previas. Por tanto, realmente no podríamos considerarlo una distribución asimétrica porque los datos están distorsionados por este elevado valor. Quizás sería recomendable cambiar el 999 por el valor nulo (cero) y ver la nueva distribución de los datos. Por otro lado, el estadístico de kurtosis es muy superior a dos, adquiriendo nuevamente un valor en torno a 20;  25.2253935767748 . Claramente se trata de un valor excesivamente elevado. Sin embargo, surge la incertidumbre si el estadístico podría volver a estar distorsionado por el valor 999, lo que con gran probabilidad será cierto. Por ello, conviene modificar este valor y volver a analizarlo para poder sacar conclusiones más consistentes a cerca de los valores anómalos. Por último, observamos que la media y la mediana sí son similares por lo que la variable está correctamente centrada. Aún así, habrá que volver a estudiar todos estos estadísticos con el cambio mencionado. '))    
  }
  else if(input$variable=="previous"){
    HTML(paste('La variable <i>previous</i> recoge el número de contactos realizados antes de esta campaña y para este cliente. Claramente se te trata de una variable numérica que toma valores únicamente enteros, ya que se cuenta el número de contactos con dicho cliente. Por este motivo, al igual que con la variable campaign, se observan oscilaciones en los gráficos que indican que esta variable no puede tomar valores decimales. <br /><br />
    
               En cuanto al primer gráfico, se puede observar que generalmente no ha habido contactos en campañas previas con la mayoría de clientes. En general, esta variable toma un rango muy pequeño de valores, ya que se observa que las observaciones se distribuyen entre los valores 0 y 6, siendo en general los valores más pequeños los más repetidos. No haber contactado en campañas anteriores con el individuo o haber contactado en una o dos ocasiones es en general, y en este orden, lo más habitual. En lo que al segundo gráfico refiere, la función de distribución señala que con prácticamente todos los clientes no había habido contactos en campañas previas, y con unos pocos había habido un contacto (se ve una menor masa en este valor). Se tiene una cola larga a la derecha, de muy poca densidad, lo que indica que más de 0/1 contactos previos no es muy habitual. Sin embargo, aunque esto se estudiará más adelante, no tiene por qué indicar anomalía en los datos, puesto que el máximo valor para esta variable es 7, y tampoco parece disparatado pensar que hayan podido contactar con cierto cliente en 7 ocasiones. Por último, en el tercer gráfico podemos distinguir las distribuciones en función del valor de la variable de salida. Se observa que para valores negativos ("no"), la función de distribución crece notablemente para el valor 0 y se reduce considerablemente en el 1, para aplanarse en valores posteriores. Si la variable de salida es "si", la masa en torno a 0 se reduce notablemente si la comparamos con la variable de salida "no". Parece que empiezan a equilibrarse, aunque no del todo, el no haber contactado con el haber contactado 1 vez. Además, también se observa que aumenta el número de individuos con los que se ha contactado 2 veces. Se tiene una curva menos marcada, con mayor masa en la cola derecha que la que se veía previamente para el otro valor. Esto es un indicador de que a través de la insistencia del operador se puede lograr una respuesta positiva por parte del cliente. <br /><br />
               
               Explicada la variable previous se observa que aparentemente no hay valores outliers. Aunque sea menos común contactar 3,4,5,6... veces con un cliente, tampoco se trata de algo extraño. Puede haber con individuos con los que se haya contactado en más ocasiones, sin que esto sea excesivo ni inisual. El rango de valores que abarca este atributo es comprensible y no hace sospechar sobre observaciones que distorsionen el estudio, aunque habrá que aplicar medidas estadísticas adecuadas para comprobarlo.<br /><br />
               
               Finalmente, se observa que la variable <i>previous</i> vuelve a ser una variable totalmente asimétrica, con un coeficiente de asimetría de 3.83176313388156. Al obtener un valor superior a la unidad y positivo, existe mayor concentración de valores a la derecha de la media que a su izquierda, por lo que en el gráfico de densidad se aprecia una larga cola a la derecha del centro. Por otro lado, el estadístico de kurtosis es muy superior a dos, adquiriendo el valor 23.1051075521029 . Sin lugar a dudas se trata de un valor excesivamente elevado, luego los datos no están próximos a la media y existe una gran heterogeneidad de los mismos. Por tanto, a pesar de que antes no habíamos detectado anomalía en los datos, quizás si que haya que suprimir algún valor considerado outlier. Por último, observamos que la media y la mediana son similares por lo que la variable sí está correctamente centrada. '))    
  }
  else if(input$variable=="emp.var.rate"){
    # social and economic context attributes
    HTML(paste(' La variable <i>emp.var.rate</i> recoge la tasa trimestral de variación de empleo. Es una variable que forma parte de un grupo de atributos económicos y sociales, por tanto debemos tener en cuenta el conjunto de atributos para obtener conclusiones más precisas.
    <br/><br/>

    La priméra gráfica muestra que la variable tiene una distribución de valores estrecha donde los valores más repetidos se corresponden con los más altos. Además, existe la hipótesis de heterogeneidad, ya que se diferencian dos grupos de valores, que se confirma de acuerdo al valor Kurtosis. Por otro lado, la segunda gráfica no nos aporta gran información. No obstante, muestra cómo los valores que toma la variable son siempre los mismos. Por último, en la tercera gráfica se muestra la información más relevante. Observamos que el número de respuestas positivas es mayor cuando la tasa de variación de empleo toma valores negativos y ocurre al contrario cuando la tasa de variación de empleo es mayor que cero.
    
    <br/><br/>
        
    Finalmente, podemos observar que la variable <i>emp.var.rate</i> no se podría llegar a considerar totalmente asimétrica dado que su coeficiente de asimetría no es mayor que uno en valor absoluto, pero sí existe un ligero sesgo hacia valores con una mayor tasa de variación. Por otro lado, como hemos mencionado anteriormente, el estadístico de kurtosis es menor a dos, de manera que una proporción importante de los datos son heterogéneos con el resto. Por último, observamos que la media y la mediana son similares por lo que podríamos decir que la variable está correctamente centrada.')) 
  }
  else if(input$variable=="cons.price.idx"){
    HTML(paste(' La variable <i>cons.price.idx</i> recoge el índice de precios al consumidor. Es una variable con una distribución estrecha, donde tenemos valores entre 92 y 95 (aproximadamente).
    
    <br/><br/>
A partir del gráfico de violín podemos interpretar que cuantas más veces aparece repetido un valor, más certeros son los resultados. De hecho, en el gráfico de densidad, observamos lo mencionado. El número de respuestas negativas es mayor cuantas más veces aparece repetido un valor. Esto quiere decir que, o bien esta variable no debe interpretarse debido a la falta de valores para las respuestas positivas, o que la variable es relevante, pero a su vez de difícil interpretación. Por otro lado, la segunda gráfica únicamente aporta la distribución de valores de manera más visual.

    <br/><br/>
        
    Finalmente, podemos observar que la variable <i>cons.price.idx</i> es prácticamente simétrica dado que su coeficiente de asimetría es cercano a cero. Por otro lado, el estadístico de kurtosis es mayor a dos, por lo que se considera homogeneidad en los datos y la falta de valores atípicos que provoquen heterogeneidad. Por último, observamos que la media y la mediana son similares, por lo que podríamos decir que la variable está correctamente centrada.')) 
  }
  else if(input$variable=="cons.conf.idx"){
    HTML(paste(' La variable <i>cons.conf.idx</i> recoge el índice mensual de confianza del consumidor. Es una variable con una distribución amplia, donde tenemos valores entre -50 y -25 (aproximadamente). Podemos plantear la hipótesis de que cuanto mayor es el índice de confianza mensual, más respuestas positivas se obtendrán.
    
    <br/><br/>
Interpretamos los gráficos de igual manera a los gráficos de la variable <i>cons.price.idx</i>.

En el gráfico de densidad, el número de respuestas negativas es mayor cuantas más veces aparece repetido un valor. Esto quiere decir que, o bien esta variable no debe interpretarse debido a la falta de valores para las respuestas positivas, o que la variable es relevante y la diferencia a favor de las respuestas positivas cuanto mayor es el índice de confianza está justificado y confirma nuestra hipótesis. Sin embargo, esta segunda opción es de difícil interpretación. Por otro lado, la segunda gráfica únicamente aporta la distribución de valores de manera más visual.

    <br/><br/>
        
     Finalmente, podemos observar que la variable <i>cons.conf.idx</i> es prácticamente simétrica dado que su coeficiente de asimetría es cercano a cero. Por otro lado, el estadístico de kurtosis es mayor a dos, por lo que se considera homogeneidad en los datos y la falta de valores atípicos que provoquen heterogeneidad. Por último, observamos que la media y la mediana son similares, por tanto podríamos decir que la variable está correctamente centrada.')) 
  }
  else if(input$variable=="euribor3m"){
    HTML(paste(' La variable <i>euribor3m</i> indica la tasa euribor a 3 meses. Es una variable que forma parte de un grupo de atributos económicos y sociales, por tanto debemos tener en cuenta el conjunto de atributos para obtener conclusiones más precisas.
    
    <br/><br/>
    
    El euríbor es un índice de referencia publicado diariamente que indica el tipo de interés promedio al que se conceden préstamos a corto plazo a terceros. Por esta razón se plantea la hipótesis de que, cuanto menor es el valor de la variable, mayor es el número de respuestas positivas.

    <br/><br/>
    
    En el primero de los gráficos observamos que se trata de una variable con distribución estrecha, donde los valores más repetidos se corresponden con los valores más altos. Observamos una distribución prácticamente idéntica a la distribución de la variable <i>emp.var.rate</i>, lo que sugiere, entre otras cosas, correlación entre variables.

    Por otro lado, la segunda gráfica no nos aporta gran información. No obstante, muestra el rango de valores en los que se mueve la variable. También, de acuerdo a el tercer gráfico, podemos observar que el número de respuestas positivas es mayor cuando la tasa euribor toma valores "pequeños" y ocurre al contrario cuando la tasa euribor toma valores "grandes".

    <br/><br/>
        
    Finalmente, podemos observar que la variable <i>euribor3m</i> no se podría llegar a considerar total asimétrica dado que su coeficiente de asimetría no es mayor que uno en valor absoluto. Por otro lado, como el estadístico de kurtosis es menor a dos podemos confirmar la existencia de dos grupos de valores diferenciados. Por último, observamos que la media y la mediana no son similares por lo que podríamos decir que la variable está sesgada hacia valores de la tasa euribor más altos. Esto reafirma nuestras conclusiones; hay una ligera asimetría, heterogeneidad y posibles valores atípicos.'))
  }
  else if(input$variable=="nr.employed"){
    HTML(paste(' La variable <i>nr.employed</i> recoge el número de empleados. Es una variable que forma parte de un grupo de atributos económicos y sociales, por tanto debemos tener en cuenta el conjunto de atributos para obtener conclusiones más precisas.
    
    <br/><br/>
    En cuanto al primer gráfico podemos observar que se trata de una variable con distribución amplia, donde los valores más repetidos se corresponden con los valores más altos. Nos damos cuenta de que la distribución es prácticamente idéntica a las distribuciones de las variables <i>emp.var.rate</i> y <i>euribor3m</i>, lo que hace que nos preguntemos, entre otras cosas, si estas variables están correlacionadas.

    Por otro lado, la segunda gráfica no nos aporta gran información. No obstante, muestra cómo los valores que toma la variable son siempre los mismos. Asimismo, en la tercera gráfica se puede observar que el número de respuestas positivas es mayor que las negativas siempre que el número de empleados sea menor que la media de la variable (aproximadamente).

    <br/><br/>
        
    Finalmente, podemos observar que la variable <i>nr.employed</i> se podría llegar a considerar ligeramente asimétrica dado que su coeficiente de asimetría es mayor que uno en valor absoluto. Sin embargo, debido a que la media y la mediana son similares, podríamos decir que la variable está correctamente centrada y por tanto no se considera que esa leve asimetría pudiera perjudicar. Por otro lado, el estadístico de kurtosis es mayor a dos por lo que se sugiere homogeneidad, pese a distinguir dos grupos diferentes de valores en el primer gráfico.'))
  }
})
```

<br/>

### {-}

## DESAGREGAR OUTLIERS

Como ha sido mencionado previamente, la eliminación de valores extremos no es la solución si estos no se deben a un error de medición o de introducción en la base de datos. Si no se debe a una equivocación, eliminarlo o sustituirlo puede modificar las inferencias que se realicen a partir de esa información ya que se introduce un sesgo en el conjunto de datos. Es decir, la variabilidad (diferencias en el comportamiento de un fenómeno) debe explicarse y no eliminarse. Sin embargo, en determinadas ocasiones esta variabilidad es difícilmente entendible y se liga directamente con una confusión, o bien por parte del usuario que ha introducido la información o bien por el investigador. En estas ocasiones sí que es necesario suprimir estas observaciones que están alterando los diferentes estadísticos calculados y las distribuciones de cada variable. Es importante analizar el posible rango de cada variable y sacar conclusiones coherentes a partir del estudio. <br/><br/>

```{r echo=FALSE}
# Variables candidatas a presentar posibles valores anómalos
variablesOutliers <- datos %>% select(duration, campaign, pdays)
```

```{r echo=FALSE}
inputPanel(
  shiny::selectInput("varOut", label="Variables con Posibles Outliers", choices = variablesOutliers%>%colnames())
)
```

```{r echo=FALSE}
renderPlot({
  datos <-data.frame(var=pull(datos%>%select(input$varOut)),y=pull(datos%>%select(y)))
  
  if (input$varOut == "duration"){
    datos <- datos%>%filter(var < 3600)
  }
  else if(input$varOut=="campaign"){
    datos <- datos%>%filter(var < 25)
  }
   p1 <- ggplot(data=datos, aes(x="", y=var)) + 
      geom_violin(color="black", fill="#a5bfde") +
      xlab("class") +
      theme(legend.position="none") +
      xlab("")+ylab(input$varOut) + 
      scale_fill_brewer(palette="Set1")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
   
  p2 <- ggplot(data=datos, aes(x=var, fill=y)) +
      geom_density(alpha=0.3)+xlab(input$varOut) + 
      scale_fill_brewer(palette="Set1")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  
  p <-  ggplot(datos, aes(x=var, y=y)) +
        scale_fill_brewer(palette="Set1")+
        geom_point() +
        geom_line(color='pink')+
        theme(legend.position="none")+xlab(input$varOut) +
        theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p3 <- ggMarginal(p, margins = 'x', color="pink", size=10)
  
        

  (p1 | p3) / p2
  
}, height=7*75)
```

```{r echo=FALSE}
renderUI({
  
  if (input$varOut == "duration"){
    datos <- datos%>%filter(datos$duration < 3600)
  }
  else if(input$varOut=="campaign"){
    datos <- datos%>%filter(datos$campaign < 25)
  }
  else if(input$varOut=="pdays"){
    datos <- datos%>%mutate(pdays = ifelse(pdays == 999, 0, pdays))
  }
  
  data<-pull(datos%>%select(input$varOut))
  
  varianza<-var(data)
  media<-mean(data)
  n<-length(data)
  curtosis<-(sum((data-media)^4)/varianza^2)/n
  asimetria<-skewness(data, na.rm = TRUE, type = 3)
  
  mediana<-median(data)
  HTML(paste("<br/>Kurtosis ",curtosis,"<br/>Coeficiente de asimetría:",asimetria,"<br/>Media:",
             media,", Mediana: ", mediana))
})
```


```{r echo=FALSE}
renderUI({
  if(input$varOut=="duration"){
    HTML(paste('Finalmente para la variable <i>duration</i> se ha decidido analizar qué ocurre cuando no se tienen valores superiores a 3600. Como ya sabemos, <i>duration</i> recoge datos en segundos del último contacto con el cliente. Por tanto, parece lógico poner el límite de 1 hora (60 minutos) como valor superior extremo. Es poco probable que una llamada telefónica de marketing se alargue durante tanto tiempo, luego valores superiores a este se van a suprimir para el estudio univariante. Tras analizar las nuevas mediciones estadísticas obtenidas, decidiremos si conviene o no aplicar este filtro. <br /><br />
               
               Se puede observar que el coeficiente de kurtosis disminuye considerablemente. De hecho, pasa del valor 23.2442057092639 al nuevo valor 18.140597873811. Esto es un indicador de que, claramente, se obtienen datos más próximos a la media y, por tanto, el conjunto de datos es más homogéneo, aunque dista mucho todavía de serlo. El coeficiente de asimetría (2.9898641470471) también se ve disminuido en casi medio punto, lo que favorece la simetría de los datos respecto a su centro. La media y la mediana no sufren grandes cambios, manteniéndose muy similares al conjunto de datos con todas las observaciones. Todo esto indica la poca centralidad de los datos y la disparidad en las observaciones. Sin embargo, es favorable suprimir aquellas observaciones que hemos considerado anómalas, puesto que los coeficientes de kurtosis y asimetría mejoran y la media y mediana prácticamente no varían, lo que demuestra que no se pierde la representatividad general de la variable. '))
  }
  else if(input$varOut=="campaign"){
    HTML(paste('La variable campaign es numérica y recoge las veces que se ha contactado con un cliente en una campaña dada. En este caso, la eliminación o no de las observaciones va a recaer considerablemente en el criterio del científico que realice el estudio. Realmente se podría contactar con los clientes tantas veces como se quiera, sin poner un número límite de llamadas. Sin embargo, es poco común, como bien reflejan los datos, contactar con ellos en un número muy elevado de ocasiones. De hecho, prácticamente todos los datos se amontonan en valores bajos, entre 0 y 5 llamadas. A partir de 20 ya parece muy poco común realizar estos contactos. Por este motivo, pondremos el umbral en 25 llamadas durante la campaña, ya que probablemente más sería una exageración. <br /><br />
               
               Se observa que el coeficiente de kurtosis disminuye de manera muy significativa en casi 20 unidades. De hecho, pasa del valor 39.9732193952114 al nuevo valor 20.1677030848009. Esto, claramente, refleja la mayor homogeneidad en los datos. Desde luego, sigue siendo un coeficiente muy alto, y muy alejado de la unidad, pero se ha conseguido mejorar la situación anterior con observaciones más próximas a la media. Además, el coeficiente de asimetría también disminuye en más de un punto, pasando de 4.76215981717883  a 3.48057445456166, lo que favorece la simetría de la variable respecto a su centro. La media y la mediana, nuevamente, no sufren grandes cambios, manteniéndose muy similares al conjunto de datos original. Creemos conveniente suprimir estas observaciones anómalas, que distan considerablemente de un escenario real y que pueden afectar negativamente a estudios posteriores. No obstante, reiterar que es una decisión subjetiva y particular pero que consideramos que favorecerá al trabajo en su totalidad.'
    ))
    }
  else if(input$varOut=="pdays"){
    HTML(paste('Se observa que las reflexiones previamente realizadas no eran ciertas. El conjunto de datos anterior sí estaba distorsionado, pero era más homogéneo que el recién obtenido. Se observa que el coeficiente de kurtosis es 79.3887619095056, exageradamente alto, lo que indica que los datos se distribuyen de forma muy heterogénea y alejada de la media. Se obtiene una larga cola a la derecha de la masa central, con algunos valores realmente elevados con respecto a la media (0.22122948431582). De hecho, existen incluso observaciones que toman valores superiores a 20. El coeficiente de asimetría es también muy elevado: 7.93895890496632. Sin embargo, a pesar de tener observaciones muy dispares y con valores realmente alejados, no concluiremos diciendo que sean outliers. Realmente no creemos que sean observaciones anómalas. Como ha sido mencionado, la variable <i>pdays</i> recoge el número de días transcurridos desde que se contactó por última vez con el cliente desde una campaña anterior. Con la mayoría de clientes, no se había contactado previamente. No obsante, tampoco es inusual haber contactado con algunos de ellos en ocasiones anteriores habiendo transcurrido varios días. Por tanto, dejaremos el conjunto de datos sin suprimir observaciones respecto a esta variable, ya que no queremos eliminar la posible variabilidad que <i>pdays</i> pueda ofrecer al estudio.'
  ))
  }
  })
```

<br/>

---

En suma, en la siguiente tabla podemos encontrar las diferentes variables apropiadamente modificadas y el dataset completo con el que trabajaremos a lo largo del trabajo.

```{r echo=FALSE}
#Conjunto de datos modificado
datos <- datos%>%mutate(pdays = ifelse(pdays == 999, 0, pdays))%>%filter(duration < 3600, campaign < 25)
datos %>% head(5) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  column_spec(column = 21, width = "3cm", background = "#ecfefe") %>%
  scroll_box(width = "900px", height = "250")
```

```{r echo = FALSE}
numerics<-datos%>%select_if(is.numeric)  
factors<-datos%>%select_if(negate(is.numeric))
```

```{r}
print(paste('Número de ejemplos / instancias totales del dataset: ', nrow(datos))) 
print(paste('Número de variables totales del dataset: ', ncol(datos)))
```
<br/>

## BREVES INTUICIONES

- Gracias a las variables *age* y *job* se intuye que la campaña de marketing no se centra en los jóvenes (individuos estudiando) ni en los ciudadanos de la tercera edad (individuos retirados), sin embargo, a la hora de contactar con usuarios de ese perfil se logró mayor proporción de llamadas exitosas.

- En la variable *marital* se observa que los individuos que se encuentran sin pareja o divorciados en el momento de la llamada proporcionan una mayor aceptación ante la campaña.

- El estudio de la variable *duration* refleja la gran importancia de este atributo sobre la variable de salida. A medida que aumenta el tiempo que dura la llamada de telemarketing, aumenta también la probabilidad de que el usuario acepte la propuesta del anunciante.

```{r echo=FALSE, warning=FALSE, message=FALSE}
durationData<-datos%>%group_by(duration)%>%count(y)

ggplot(durationData, aes(x=duration, y=n, group=y, color=y)) +
    geom_line() +
    geom_point() +
    scale_color_manual(values = paletaContinua) +
    ylab("Llamadas") +
    transition_reveal(duration)

anim_save("transition.gif")
```

Gracias a la gráfica anterior, se puede observar claramente como en llamadas de corta duración la proporción de llamadas exitosas es minoritaria, sin embargo, a medida que la duración de la llamada aumenta, el número de llamadas es reducido al mismo tiempo que la proporción de llamadas aceptadas predominan. 

- La variable *previous* intuye también que la insistencia puede provocar una respuesta positiva por parte del cliente. Aparentemente, se ha contactado en mayores ocasiones con clientes que terminan respondiendo "yes". 

Con el objetivo de analizar con profundidad estas intuiciones, se realizará un análisis multivariante en la que se pretenderá proporcionar algún tipo de razonamiento.

<br/>

# ANÁLISIS MULTIVARIANTE

El propósito del análisis multivariante es medir, explicar y predecir el grado de relación que existe entre las variables. El carácter multivariante del análisis radica no sólo en el número de variables, sino en las múltiples combinaciones existente entre estas. A la hora de realizar el análisis multivariante comenzaremos haciendo un análisis previo con el objetivo de seleccionar un conjunto de variables a las que les aplicaremos las técnicas de análisis avanzadas. 


## ANÁLISIS PREVIO

En este análisis previo se seleccionará un conjunto de variables con característica propias, es decir, se tratará de seleccionar un conjunto de variables no redundantes. Asimismo, el objetivo de estas variables será el de verificar algunas de nuestras intuiciones o la de proporcionar información de gran relevancia para lograr nuestro objetivo; modelizar el éxito de la campaña de marketing. Es necesario matizar que este análisis previo solo lo podremos llevar a cabo con las variables numéricas.


###  MATRIZ DE VARIANZAS Y COVARIANZAS

Comenzamos calculando la matriz de covarianzas y varianzas del conjunto de todas las variables. Una vez hemos calculado esta matriz, analizaremos su rango y en el caso de que este no sea el máximo, es decir, el número de variables del dataset, podremos concluir que existen variables redundantes dentro del conjunto de datos. 

```{r echo=FALSE}
S<-cov(numerics)
rank<-rankMatrix(S)
print(paste('El rango de la matriz de varianzas y covarianzas es: ',rank))
print(paste('Existencia de variables númericas redundantes: ',rank!=ncol(numerics)))
```

Se puede observar que el rango de la matriz de covarianzas y varianzas es igual al número de variables numéricas del conjunto. Por consiguiente, por ahora no observamos la existencia de variables redundantes.

<br/>
<a name="correlation"></a>

### MATRIZ DE CORRELACIONES {.tabset .tabset-pills}

Continuamos con el objetivo de detectar variables redundantes dentro del conjunto de variables numéricas, por ello, calculamos la matriz de correlaciones donde podemos observar la relación dos a dos de todas las variables. Además, vamos a modificar las variables categóricas y numéricas con la técnica *ONE HOT* para poder crear una matriz de correlaciones que contenga todas las variables. Cabe mencionar que la técnica *ONE HOT* crea una variable binaria por cada categoría que tiene cada variable.

#### Correlacions con variables numéricas

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Matriz de correlaciones</a>

```{r fig.align="center",echo=FALSE, warning=FALSE}
correlaciones1 <- round(cor(numerics), 2)
#Matriz de correlaciones
p<-ggcorrplot(correlaciones1, hc.order = TRUE, type = "lower", lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))
p
```

Gracias a la matriz de correlaciones se pueden observar algunas variables que están altamente correlacionadas entre ellas, por ello, podemos intuir que algunas pueden llegar a ser redundantes.

Entre las variables más correladas podemos destacar las siguientes tres variables:

- *emp.var.rate*.
- *euribor3m*.
- *nr.employed*.

Se puede observar como la correlación entre estas 3 variables se aproxima a 1, es decir, son variables que se pueden llegar a explicar unas a otras. Además, estas 3 variables también correlan positivamente con la variable *cons.price.idx*, al mismo tiempo que correlan negativamente con las variables *previous* y *pdays*. Cabe destacar la correlación entre las tres mencionadas con *cons.price.idx*, presentando correlaciones ligeramente elevadas (todas ellas superiores al valor 0.5, lo que hace sospechar sobre una posible correlación entre estos cuatro atributos). Más adelante, en el análisis PCA, se analizarán maneras de combinar estas variables redundantes en componentes que recojan su variabilidad. 

Con el objetivo de conslidar la idea anterior sobre las tres variables seleccionadas, se ofrece la opción de ver la correlación entre dos de las variables en función de las variables categóricas. 

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Correlación entre par de variables seleccionadas en función de categoría</a>
<br/>
```{r echo=FALSE}
inputPanel(
  shiny::selectInput("corr1", label="Numérica 1", choices = c("emp.var.rate","euribor3m","nr.employed")),
  shiny::selectInput("corr2", label="Numérica 2", choices = c("emp.var.rate","euribor3m","nr.employed")),
  shiny::selectInput("fill", label="Categórica", choices = factors%>%colnames())
)
```

```{r echo=FALSE}
renderPlot({
  datos<-data.frame(var1=pull(datos%>%select(input$corr1)),var2=pull(datos%>%select(input$corr2))                  ,var3=pull(datos%>%select(input$fill)))

     ggpairs(datos,columns = 1:2, ggplot2::aes(colour=var3))
  
}, height=7*75)
```
<br/>

Gracias a las anteriores gráficas se puede observar que la existencia de una variable redundante entre las 3 variables mencionadas es cierta, ya que en la mayoría de los factores del dataset las tres están altamente correlacionadas. Por tanto, podríamos tomar solo una de ellas para explicar el resto. Habría que considerar también las características de *cons.price.idx*, que aunque su correlación sea menor, no cabe duda que aportará información al estudio. 

A través de los gráficos proporcionados se aprecia una tendencia similar de las variables numéricas frente a las distintas variables categóricas. Se observan también altas correlaciones, indicador de lo previamente mencionado: hay redundancia entre variables. Pongamos, por ejemplo, que se selecciona la variable numérica 1 *emp.var.rate*, la segunda *euribor3m* y finalmente la categoría *marital*. Podemos observar un comportamiento muy similar en ambos gráficos, con subidas y bajadas prácticamente iguales y correlaciones muy elevadas para cada una de las categorías del atributo. Desde luego, este sería un buen ejemplo para apreciar los hechos previamente mencionados. 

Por otra parte, destaca también la correlación entre previous y pdays (0.56). El valor no es tan elevado como los previamente estudiados, pero no por ello se debe descartar una posible redundancia entre estos dos atributos. Además, posteriormente se observará que ambas tienen cierta incidencia sobre las categorías de la variable *poutcome*, lo que reitera el hecho de que pueden ser dos posibles atributos candidatos a presentar redundancia. Además, ambos atributos presentan correlaciones negativas frente a las cuatro variables recién comentadas. Se podrá ahondar más en estos hechos en el análisis que incluye también las variables categóricas. 


#### Correlaciones entre variables numéricas y categóricas

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Técnica One Hot</a>

A continuación, hacemos uso de la técnica *One Hot* para transformar las variables categóricas en numéricas.

```{r warning=FALSE, message=FALSE}
library(caret)
dummy <- dummyVars(" ~ .", data=factors)
one_hot <- data.frame(predict(dummy, newdata=factors))
#Como la variable eliminamos yno y mantenemos yyes, ya que en este caso es suficiente
one_hot<-one_hot%>%select(-yno)
one_hot<-one_hot%>%mutate(y=yyes)%>%select(-yyes)
#Unimos las variables categóricas y las numéricas
datos_OneHot<-cbind(numerics,one_hot)
```

Una vez realizada la técnica *One Hot*, continuamos calculando la matriz de correlaciones entre todas las variables.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Matriz de correlaciones</a>

```{css, echo=FALSE}
/*Centrar gráficas plotly*/
.center {
  display: table;
  margin-right: auto;
  margin-left: auto;
}
```
<div class = 'center'>
```{r echo=FALSE}
correlaciones2 <- round(cor(datos_OneHot), 2)
p<-ggcorrplot(correlaciones2, hc.order = TRUE, type = "lower",colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))+
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()
        )

ggplotly(p)
```
</div>

Gracias a la anterior matriz de correlaciones podemos observar qué variables aportan mas información. Por ello, vamos a analizar cuál de las tres variables seleccionadas anteriormente aportan mayor información al resto de variables.

```{r echo=FALSE}
euribor3m<-data.frame(correlaciones2)%>%select(euribor3m)%>%sum()
emp.var.rate<-data.frame(correlaciones2)%>%select(emp.var.rate)%>%sum()
nr.employed<-data.frame(correlaciones2)%>%select(nr.employed)%>%sum()
print(paste("corr(euribor3m,...)",euribor3m,",corr(emp.var.rate,...): ",emp.var.rate,",corr(nr.employed,...): ",nr.employed))
```

Observamos que la variable *euribor3m* es la variable que mas correlada esta con el conjunto de datos entre las tres variables mencionadas. Además, dentro de ese conjunto seleccionado es la variable que mejor explica el resto.  <br/>

Por otra parte, en cuanto a las correlaciones menos elevadas, se pueden observar las siguientes variables relacionadas:

- *pdays*
- *poutcomesuccess*, es decir, la categoría success correspondiente a la variable poutcome.
<br/><br/>
- *poutcomefailure*, es decir, la categoría nonexistent correspondiente a la variable poutcome.
- *previous*

En la sección anterior habíamos estudiado una posible redundancia entre las variables *pdays* y *previous*, ya que presentaban una correlación de 0.56. Este valor no es excesivamente elevado y quizás no sea determinante para establecer repetición entre atributos. No obstante, al añadir las categorías al estudio observamos que ambas correlan positiva y fuertemente con las categorías de la variable *poutcome*, en especial  *failure* con *previous* (0.68) y *success* con *pdays* (0.74). 


```{r echo=FALSE}
inputPanel(
  shiny::selectInput("num1", label="Numérica 1", choices = c("previous", "pdays")),
  shiny::selectInput("num2", label="Numérica 2", choices = c("previous", "pdays")),
  shiny::selectInput("cat", label="Categórica", choices = c("poutcome"))
)
```

```{r echo=FALSE}
renderPlot({
  datos<-data.frame(var1=pull(datos%>%select(input$num1)),var2=pull(datos%>%select(input$num2)), var3=pull(datos%>%select(input$cat)))
  
   ggpairs(datos,columns = 1:2, ggplot2::aes(colour=var3))
  
}, height=7*75)
```

Aunque no sean correlaciones especialmente significativas, sí que nos aportan cierta información al estudio, sobre todo de cara a la aplicación de posteriores técnicas. Es cierto que *pdays* y *previous*, ambas juntas no parecen tener especial correlación con *poutcome*. Sin embargo, como ha sido mencionado, la correlación entre estos dos primeros atributos no debe pasarse por alto. Además, recordar que ambas correlaban positivamente también con categorías específicas de la variable *poutcome* y negativamente con las primeras variables estudiadas, luego serán intuiciones que se tendrán que tener en cuenta a lo largo del estudio. 

### {-}

---

<br/>

## VARIABLES CONSIDERADAS REDUNDANTES O DE GRAN INTERÉS

Para terminar con el análisis previo al uso de técnicas de análisis multivariantes avanzadas, se debe mencionar que hay algunas variables con gran relevancia como *duration*, la cual tiene la mayor correlación con la variable *y* pero tiene la peculiaridad de que no presenta una correlación significativa con el resto de variables. Sin embargo, como la duración de la llamada no se conoce antes de realizar y una vez finalizada la llamada es obvio que se conoce el resultado de la misma, no nos centraremos tanto en esta variable ya que de cara a futuras campañas se pretende lograr información predictiva realista.

```{r echo=FALSE}
dur<-pull(data.frame(correlaciones2)%>%select(y))
dur<-data.frame(rbind(dur))
colnames(dur)=data.frame(correlaciones2)%>%select(y)%>%rownames()
dur %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  scroll_box(width = "900px", height = "250")
```

<br/>

Por otro lado, como ya se ha mencionado las variables *emp.var.rate* y *nr.employed* se consideran redundantes y tomamos la decisión de centrarnos más en la variable *euribor3m*.

<br/>


##  DISTANCIAS MULTIVARIANTES

Un procedimiento alternativo para estudiar la variabilidad de las observaciones es utilizar el concepto de distancias entre puntos. Además, el concepto de distancia entre puntos será importante en las secciones siguientes ya que algunas técnicas de Análisis Multivariante están basadas en criterios geométricos y en la noción de distancia entre individuos. <br/>

###  DISTANCIA DE MAHALANOBIS

La distancia más utilizada es la *euclídea* pero tiene el inconveniente de no ser invariante por cambios de escala, es decir, depende de las unidades de medida de las variables. Una manera de evitar el problema de las unidades es dividir cada variable por un término que elimine el efecto de la escala. Esto conduce a la familia de métricas euclídeas ponderadas, donde se incluye la distancia de *Mahalanobis* (*dM*). Esta distancia es invariante por transformaciones lineales no singulares de las variables, en particular cambios de escala, y tiene en cuenta las correlaciones entre las variables. Su utilidad radica en que es una forma de determinar la similitud entre dos variables aleatorias multidimensionales. 

La aplicación principal que le daremos a la distancia de *Mahalanobis* es la de detectar outliers en cuanto al Análisis Multivariante. Es decir, en la primera sección de este estudio se han detectado atributos anómalos en base a una única variable. En esta parte, se pretende analizar la existencia de observaciones extrañas bivariables, es decir, considerando las correlaciones que presentan los atributos dos a dos. De este modo se conseguirán detectar oultiers multivariantes. Finalmente se tratarán de identificar observaciones extrañas considerando todas las características del conjunto.

>Hay que tener en cuenta que este conjunto de datos está compuesto por un gran número de observaciones. De hecho, se consideran en torno a cuarenta mil individuos. Por este motivo, realizar un gráfico en representación a estos sería un desperdicio, bien por coste computacional y bien por visibilidad, ya que al representar tantas observaciones se amontonarían entre ellas y se acabaría obteniendo una nube imposible de interpretar. Por este motivo, se va a seleccionar una muestra aleatoria del conjunto de datos original, con el objetivo de que esta sea representativa y se puedan lograr algunas intuiciones en cuanto a su distribución. No obstante, no hay que pasar por alto que se trata de un problema no balanceado, en el que se tienen muchas más respuestas “no” que “yes”. Por este motivo, y con la intención de tratar de mantener la representatividad de los datos, vamos a restringir la aleatoriedad a las proporciones “yes” y “no” de la variable de salida y que hemos analizado en el Análisis Univariante. De esta manera, no perderemos observaciones importantes y se mantendrá la especificidad de los datos. Así, generaremos una muestra de 500 individuos.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Estudio Bivariante</a>

```{r echo=FALSE}
conjuntoNo <- datos %>% filter(y=="no") 
conjuntoYes <- datos %>% filter(y=="yes") 
n = 500

muestraNo<- conjuntoNo %>% sample_n(size=round(0.8873458*n, 0), replace = FALSE) 
muestraYes<- conjuntoYes %>% sample_n(size=round(0.1126542*n, 0), replace = TRUE)

muestraFinal <- rbind(muestraNo, muestraYes)

# De esta muestra selecciono únicamente las numéricas
numericasMuestra <- muestraFinal%>%select_if(is.numeric)  
#Estandarizo
NumericMuestraEstandar <- numericasMuestra %>% mutate_all (~ ( scale (.)%>% as.vector ))
```


```{r echo=FALSE}
inputPanel(
  shiny::selectInput("seleccion1", label="Variable 1", choices = numericasMuestra%>%colnames(), selected = "duration"),
  shiny::selectInput("seleccion2", label="Variable 2", choices = numericasMuestra%>%colnames(), selected = "pdays")
)
```


```{r fig.align="center", echo=FALSE}
renderPlot({
  
  datos2<-data.frame(s1=pull(numericasMuestra%>% select(input$seleccion1)),s2=pull(numericasMuestra%>%select(input$seleccion2)))
  muestra2 <- datos2 %>% select(s1,s2)
  
  vectorMedias2 <- sapply(muestra2, mean)
  dM2 <- mahalanobis(muestra2, vectorMedias2, cov(muestra2))
  cutoff <- qchisq(p = 0.95 , df = ncol(muestra2))
  
  # Finding ellipse coordiantes
  ellipse <- car::ellipse(center = vectorMedias2 , shape = cov(muestra2) , radius = sqrt(qchisq(p = 0.95 , df = ncol(muestra2))) ,
                        segments = 150 , draw = FALSE)
  
  # Ellipse coordinates names should be same with air data set
  ellipse <- as.data.frame(ellipse)
  colnames(ellipse) <- colnames(muestra2)
  
  figure <- ggplot(muestra2 , aes(x =s1 , y = s2)) +
         geom_point(size = 2) +
         geom_polygon(data = ellipse , fill = "lightblue" , color = "lightblue" , alpha = 0.5)+
         geom_point(aes(vectorMedias2[1] , vectorMedias2[2]) , size = 5 , color = "blue") +
         geom_text( aes(label = row.names(muestra2)) , hjust = 1 , vjust = -1.5 ,size = 2.5 ) + xlab("Variable 1") + xlab("Variable 2")
  
  ggMarginal(figure, type="boxplot")
  }, height=7*75)
```

```{r echo=FALSE}
renderUI({
    HTML(paste('A través de los gráficos superiores se pueden analizar las observaciones anómalas o extrañas considerando pares de conjuntos de variables. Se realizan gráficos de dispersión en los que el punto azul muestra el punto central y la elipse recoge el conjunto de individuos no extraños. Los puntos negros son las observaciones para el par de variables en cuestión. Todos los puntos que caen en el exterior de la elipse son candidatos a ser valores outliers. Del mismo modo, en los bordes superior y derecha se representa la información a través de diagramas de cajas. Estos diagramas son los correspondientes al estudio individual de cada uno de las variables que se están considerando. Todos los puntos que se sitúan fuera de los bigotes del diagrama de cajas son aquellos que se deben estudiar por ser extraños. 
               <br/><br/>
               
               Si seleccionamos por ejemplo los atributos <i>age</i> y <i>duration</i>, observamos que hay un número considerable de individuos extraños. Numerosas observaciones se sitúan en el exterior de la elipse, al igual que alejados de los bigotes del diagrama de cajas. Todos ellos serán candidatos a ser valores extraños. Esto se podría ver acentuado por la presencia de valores anómalos individuales en la variable <i>duration</i>. Manteniendo <i>age</i> y seleccionando <i>campaing</i>, obervamos que el número de valores anómalos se reduce. Es decir, hay menos individuos extraños para este par de variables. Eligiendo como segunda variable <i>cons.price.idx</i> también se analiza una disminución del número de individuos extraños, además de un aumento del radio de la elipse. Si ahora cambiamos la primera variable y tomamos <i>pdays</i> vemos que tampoco parece haber un excesivo número de outliers. Finalmente, pongamos por ejemplo <i>euribor3m</i> y <i>nr.emplyed</i>. No se observan prácticamente valores anómalos, ni en su conjunto, ni individualmente. 
               <br/><br/>
               
               Todos estos gráficos, estudiados en pares de variables, nos ayudan a entender la distribución de los individuos en función de estas. Es decir, seleccionados dos atributos permiten analizar observaciones posiblemente anómalas. Son gráficos que proporcionan gran información y cuyas conclusiones se tendrán que aplicar para continuar con el estudio. 
             <br/><br/>
             
             No obstante, estamos hasta este punto estamos restringiendo el análisis únicamente al estudio bivariable. Sin embargo, el número de atributos que se tendrían que considerar es muy superior. Por este motivo, quizás estamos detectando valores extraños que en conjunto, quizás no lo sean. Por este motivo, antes de suprimir directamente las observaciones se deberá hacer un estudio conjunto de todos los atributos. No cabe duda que el análisis de pares de variables es realmente útil para entender las relaciones entre variables e individuos, pero hay que complementarla con un estudio conjunto global.
               '))
  
    })
```


<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Estudio Completo</a>

En esta sección se estudiará el análisis completo de la distancia de *Mahalanobis*. Para ello se graficará un histograma y un diagrama de cajas, que permitirán detectar valores anómalos considerando el conjunto completo de variables numéricas. Como ha sido previamente mencionado, todos los individuos que se alejen de los bigotes del diagrama de cajas serán considerados atributos extraños, y de esta manera, suprimidos para continuar con el resto del estudio. 

```{r echo=FALSE}
#Distancia mahalanobis entre todas las variables
vectorMediasTot <- sapply(numericasMuestra, mean)
dMTot <- mahalanobis(numericasMuestra, vectorMediasTot, cov(numericasMuestra))
```

```{r fig.align="center", echo=FALSE}
# Fill with plots
par(mfrow=c(1,2))
hist(dMTot , breaks=30 , border=F , col="lightblue", xlab="Distancias de Mahalanobis" , main="")
boxplot(dMTot , xlab="Distancias de Mahalanobis" , col="lightblue" , las=2)
```

Se observa que hay un número elevado de individuos que se sitúa en el exterior de los bigotes del diagrama de cajas. Por tanto, los consideraremos valores anómalos. De hecho, hay un número considerable de observaciones más próximas a los bigotes, pero hay dos individuos cuya distancia de *Mahalanobis* es excesivamente grande y se alejan de manera considerable. Hay que recordar que esto era simplemente una muestra del dataset original para poder graficar bien los individuos e interpretarlos. Sin embargo, en la próxima sección lo tendremos que implementar para todos los individuos con el objetivo de suprimir todos los valores extraños.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Desagregar Outliers Multivariantes</a>

Finalmente, lo que se va a hacer es desagregar los outliers multivariantes encontrados, de tal manera que el estudio próximo no se vea distorsionado por estos. Las características del conjunto de datos resultante las veremos a continuación. 


```{r echo = FALSE}
vM <- sapply(numerics, mean)
dM <- mahalanobis(numerics, vM, S)

# Cutoff value for ditances from Chi-Sqaure Dist. with p = 0.95 df = 2 which in ncol(air)
cutoff <- qchisq(p = 0.95 , df = ncol(numerics))

## Display observation whose distance greater than cutoff value
outliers <- numerics[dM > cutoff, ]
sinOutliers <- datos[dM < cutoff, ]
```

Generamos una representación de la muestra, y a continuación, a esa muestra le añadimos en su misma proporción la cantidad de datos outliers seleccionados. Para representar los individuos lo que se hace es proyectar las observaciones considerando las dos primeras componentes principales que se obtendrían en el análisis PCA, ya que serán las que mayor variabilidad de los datos recojan. De esta manera podremos intuitivamente observar la calidad de los individuos detectados como extraños, y confirmar o desmentir su rareza. 

<br/>

```{r echo = FALSE}
# Muestra con todos -> cojo una muestra
conjuntoNo <- sinOutliers %>% filter(y=="no") 
conjuntoYes <- sinOutliers %>% filter(y=="yes") 
n = 500

muestraNo<- conjuntoNo %>% sample_n(size=round(0.8873458*n, 0), replace = FALSE) 
muestraYes<- conjuntoYes %>% sample_n(size=round(0.1126542*n, 0), replace = TRUE)

muestraFinal <- rbind(muestraNo, muestraYes)

# Ahora a esa muestra le añado los outliers proporcionalmente ; en torno a 60
muestraFinalOut <- rbind(muestraFinal %>% select_if(is.numeric), outliers %>% sample_n(size = 60))
```

En este primer gráfico se muestra el conjunto de datos sin outliers.

*** 

```{r fig.align="center", echo = FALSE}
# Representación de individuos
res.pca1 <- PCA(muestraFinal %>% select_if(is.numeric), ncp=5, graph = FALSE)

fviz_pca_ind(res.pca1,col.ind="cos2",col.ind.sup = "black", gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```

<br/>

En el segundo gráfico se muestra el conjunto de datos con los outliers incluidos (original).

***

Sin embargo, para detectar bien los outliers lo que se ha hecho es, a la muestra sin outliers, sumarle proporcionalmente estas observaciones extrañas. Esto lo hacemos para poder realizar bien la comparación, ya que a través de muestras aleatorias se vería distorsionado el conjunto

```{r fig.align="center", echo = FALSE}
# Representación de individuos sin outliers
res.pca2 <- PCA(muestraFinalOut %>% select_if(is.numeric), ncp=5, graph = FALSE)

fviz_pca_ind(res.pca2,col.ind="cos2",col.ind.sup = "black", gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```

Se observa que realmente las observaciones seleccionadas como outliers multivariantes sí lo eran. La gran mayoría de estas se situaban en el segundo cuadrante del gráfico, mostrando una cola izquierda más extensa y no permitiendo diferenciar tan claramente los grupos. Tras eliminar outliers, los grupos de individuos se seleccionan mejor y se obtienen mejores resultados.

```{r}
print(paste('El número de outliers multivariantes es: ', nrow(outliers))) 
print(paste('Número de ejemplos / instancias totales del dataset: ', nrow(sinOutliers))) 
```

Finalmente se eliminan las instancias consideradas outliers y se muestra un breve resumen del conjunto de datos resultante. Se han suprimido en torno a 5.000 individuos que presentaban características dispares y que no habían sido detectados en el análisis univariante. A continuación se trabajará con el nuevo conjunto de datos, 

```{r echo = FALSE}
sinOutliers%>% head(5) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>% 
  scroll_box(width = "900px", height = "250")
```

<br/>
Cabe mencionar que el estudio de análisis de correlaciones se podría realizar una vez hemos eliminado estos valores anómalos. No obstante, consideramos que los supuestos outliers podrían dar gran información sobre la correlación entre variables en casos extremos  y por ello realizamos este procedimiento. 

## ANÁLISIS DE PCA

El Análisis de Componentes Principales (PCA) es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información. En numerosas ocasiones, como hemos visto en la sección introductoria del análisis multivariante, existen variables altamente correladas, y por tanto, redundantes. El método de PCA permite así condensar la información aportada por múltiples variables en solo unas pocas componentes. Esto lo convierte en un método muy útil de aplicar a la previa utilización de otras técnicas como análisis factorial, clustering…

En cuanto al proceso fundamental de este método, el PCA identifica aquellas direcciones en las que la varianza es mayor. Como la varianza de una variable se mide en su misma escala elevada al cuadrado, si antes de calcular las componentes no se estandarizan todas las variables para que tengan media 0 y desviación estándar 1, aquellas variables cuya escala sea mayor dominarán al resto. Por este motivo, resulta indispensable estandarizar todas las variables antes de aplicar el análisis PCA. Además, al trabajar con varianzas, el método PCA es altamente sensible a outliers. Por este motivo se ha realizado un análisis profundo de limpieza de datos, con el objetivo de prevenir posibles problemas que pudieran surgir si el conjunto de datos no estuviera altamente pulido.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Objetivo de la técnica</a>

El problema que se desea resolver es cómo encontrar un espacio de dimensión más reducida que represente adecuadamente los datos. Cuando contamos con un gran número de variables cuantitativas posiblemente correlacionadas (emp.var.rate, euribor3m, nr.employed...), el PCA permite reducirlas a un número menor de variables transformadas (componentes principales) que expliquen gran parte de la variabilidad en los datos. Cada dimensión o componente principal generada por el PCA será una combinación lineal de las variables originales, y serán además independientes o no correlacionadas entre sí. 

Para comenzar con el análisis PCA parece lógico seguir el siguiente esquema.

1. Centrar los datos.
2. Estudiar el número de componentes principales que se deben seleccionar.
3. Análisis PCA

<br/>

#### Estandarización de los datos

---
Se obtienen los datos estandarizados, de tal modo que el nuevo conjunto de datos es el que sigue:
<br/>
```{r echo = FALSE}
numerics <- sinOutliers%>%select_if(is.numeric)  
factors<-sinOutliers%>%select_if(negate(is.numeric))
```

> También se puede seleccionar la estandarización del conjunto de datos a través de parámetros en las funciones del análisis PCA. 

```{r echo = FALSE}
# Estandarizo los datos
numericsEstandarizados <- numerics %>% mutate_all (~ ( scale (.)%>% as.vector ))
#numericsEstandarizados <- cbind(numericsEstandarizados,datos_OneHot$y)
datosEstandarizados <- cbind(numericsEstandarizados,factors)

datosEstandarizados %>% head(5) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  column_spec(column = 21, width = "3cm", background = "#ecfefe") %>%
  scroll_box(width = "900px", height = "250")
```

<br/>

#### Selección del número de Componentes Principales

***
Tras haber estandarizado los datos, es esencial conocer el número de componentes principales que se deben seleccionar para llevar a cabo el análisis PCA. Este valor se calcula visualmente a través del gráfico Scree Plot, en el que se construyen las componentes principales y se pintan los valores propios para poder decidir cuántos seleccionar. El Scree Plot siempre muestra los valores propios en una curva descendente, ordenándolos de mayor a menor. Según el Scree Test, el "codo" del gráfico en el que los valores propios parecen nivelarse es donde se establece el número de componentes principales a utilizar. Así, los factores o componentes situados a la izquierda de este punto deben considerarse significativos.

Las componentes principales son una combinación lineal normalizada de las variables originales de un conjunto de datos. Al calcularse sobre variables estandarizadas, los componentes principales son autovectores que se toman de la matriz de correlaciones. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. 

$$
Z_{i}=ϕ_{1i}X_{1}+ϕ_{2i}X_{2}+...+ϕ_{pi}X_{p}
$$

Los términos $ϕ_{1i}X_{1}+ϕ_{2i}X_{2}+...+ϕ_{pi}X_{p}$ reciben en el nombre de *loadings* y son los que definen a la componente. Los *loadings* pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer qué tipo de información recoge cada una de las componentes.


```{r echo = FALSE}
pca <- prcomp(numericsEstandarizados, scale = TRUE)
```

La elección del número de componentes se realiza de manera que la primera componente principal sea la que mayor varianza recoja, la segunda debe recoger la máxima variabilidad no recogida por la primera, y así sucesivamente, eligiendo un número que recoja un porcentaje suficiente de varianza total. 

1. La primera componente principal ($Z_{1}$) es aquella cuya dirección refleja o contiene la mayor variabilidad en los datos (por lo que esta componente será la que más información contenga). 

2. La segunda componente principal ($Z_{2}$) será una combinación lineal de las variables, tal que recoge la segunda dirección con mayor varianza de los datos, pero que no esté correlacionada con $Z_{1}$. Esta condición es equivalente a decir que la dirección de $Z_{2}$ ha de ser ortogonal respecto a $Z_{1}$. Y así sucesivamente.

Analizar con detalle el vector de *loadings*  puede ayudar a interpretar qué tipo de información recoge cada componente. Por ejemplo, la primera componente es el resultado de la siguiente combinación lineal de las variables originales:

```{r echo = FALSE}
pca$rotation[,1] 
```

$$
Z_{1} = -0.063240740Age -  0.007972184Duration + ...  -0.477423934 Euribor3m -0.455731088Nr.employed
$$

Observamos que los atributos finales tienen un peso mayor que los inicialmente considerados. Para seleccionar el número adecuado de componentes principales, se realizan los siguientes gráficos.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
res.pca <- PCA(numericsEstandarizados, ncp=6, graph = FALSE)
screePlot <- fviz_screeplot(res.pca, ncp=10, addlabels=TRUE, barcolor = "skyblue", barfill = "skyblue",
    linecolor = "steelblue") +labs(title = "Varianzas - PCA",
         x = "Componentes Principales", y = "% de Varianzas")
screePlot
```

```{r fig.align="center",echo=FALSE, warning=FALSE}
prop_varianza <- pca$sdev^2 / sum(pca$sdev^2)
prop_varianza_acum <- cumsum(prop_varianza)
var_acum <- ggplot(data = data.frame(prop_varianza_acum, pc = 1:10),
       aes(x = pc, y = prop_varianza_acum, group = 1)) +
  geom_point() +
  geom_line() +
  geom_label(aes(label = round(prop_varianza_acum*100,2))) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Prop. varianza explicada acumulada")
var_acum  
```


Siendo el objetivo del PCA reducir la dimensionalidad, suele ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos. Sin embargo, en este ejemplo concreto observamos que con las dos primeras componentes principales solo se consigue explicar el 53.16% de la varianza total. En función de la especificidad que se requiera en el estudio, se tratará de obtener un % de varianza superior o inferior. No obstante, el umbral se establecerá en torno al 80% de la varianza explicada. En este caso, vemos que con 5 componentes principales se logra un 82.91%. Si aumentamos el número de componentes principales a 6, la varianza explicada será del 90.33%. Reiterar el hecho de que depende el investigador qué decisión tomar. Por una parte, se podría favorecer la inclusión de un número menor de componentes frente a una menor explicación de la varianza, o al contrario, tratar de explicar toda la variabilidad. En este caso, lo que está claro es que se deben seleccioner entre 5 y 6 componentes principales para llevar a cabo el análisis. 


```{r fig.align="center",echo=FALSE, warning=FALSE}
par(mfrow=c(1,2))
res.pca6 <- PCA(numericsEstandarizados, ncp=6, graph = FALSE)
res.pca5 <- PCA(numericsEstandarizados, ncp=5, graph = FALSE)

var5 <- get_pca_var(res.pca5)
cor5 <- ggcorrplot(var5$cos2, col = c("#c75fe7","#f7e9fc" ,"#88edeb"), tl.col = "darkblue")

var6 <- get_pca_var(res.pca6)
cor6 <- ggcorrplot(var6$cos2, col= c("#c75fe7","#f7e9fc" ,"#88edeb"), tl.col = "darkblue")

cor5+cor6
```


Finalmente, escogemos la selección de 5 componentes principales. A pesar de que la variabilidad explicada sea mayor añadiendo una componente más, el propósito principal del Análisis PCA es reducir la dimensión del número de variables. Por este motivo, y considerando que el número de variables numéricas inicial era 10, creemos favorecedor seleccionar 5 componentes. De este modo, se conserva gran parte de la variabilidad inicial y se consigue reducir notablemente el número original de atributos. Además, a través de los gráficos superiores se observa que para la sexta componente la contribución de variables no es muy elevada. Todo ello confirma la elección previa. Por último, en cada una de las dimensiones se analiza la contribución de las diferentes variables del conjunto de datos. Se profundizará a continuación en todos estos aspectos a través del Análisis de Componentes Principales.

<br/>

### Análisis PCA {.tabset .tabset-pills}

***
 En esta sección lo que se pretende es ver gráficamente la influencia que cada una de estas componentes principales tiene sobre las variables estudiadas. Como ha sido mencionado, la variabilidad que las componentes principales (PC) aportan va decreciendo con la inclusión de estas. Es decir, la primera componente seleccionada es la que ofrece mayor varianza, y así sucesivamente. En suma, se pretende analizar las variables desde el punto de vista de las dimensiones de estudio, desde el origen de ejes, relación con las dimensiones etc.

Para ello se utilizarán diversos gráficos, en especial aquellos que muestran cómo las variables iniciales participan en la construcción de los PC. En los gráficos, para estos vectores (variables), nos fijamos en su longitud y en el ángulo con respecto a los ejes de las componentes principales y entre ellos mismos:

- Ángulo: cuanto más paralelo es un vector al eje de una componente, más ha contribuido a la creación de la misma. Con ello se obtiene información sobre qué variable(s) ha sido más determinante para crear cada componente, y si entre las variables hay correlaciones. Ángulos pequeños entre vectores representa alta correlación entre las variables implicadas ; ángulos rectos representan falta de correlación, y ángulos opuestos representan correlación negativa.

- Longitud: cuanto mayor es la longitud de un vector, mayor variabilidad de dicha variable está contenida en la representación de las dos componentes, es decir, mejor está representada su información en el gráfico. Numéricamente viene representado por el $cos2$.

<br/>

#### Representación de Individuos {.tabset .tabset-pills}

El $cos2$ de cada individuo respecto a cada componente indica la calidad de la representación del individuo por dicha componente. La suma de todos los cosenos es igual a uno. Este gráfico resulta interesante ya que se puede ver cómo se distribuyen los individuos y es fácil detectar valores anómalos. 

Al igual que en la sección de distancias, reduciremos considerablemente el conjunto de observaciones para poder hacer representaciones de individuos. El tratamiento de la muestra será idéntico. Se generará una muestra de 500 individuos, seleccionados proporcionalmente según la variable de salida *y* de tal manera que se mantenga, al máximo, la representatividad del conjunto de datos original.


```{r echo=FALSE}
conjuntoNo <- sinOutliers %>% filter(y=="no") 
conjuntoYes <- sinOutliers %>% filter(y=="yes") 
n = 500

muestraNo<- conjuntoNo %>% sample_n(size=round(0.8873458*n, 0), replace = FALSE) 
muestraYes<- conjuntoYes %>% sample_n(size=round(0.1126542*n, 0), replace = TRUE)

muestraFinal <- rbind(muestraNo, muestraYes)

#Represento la muestra en una tabla
muestraFinal %>% head(5) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  column_spec(column = 21, width = "3cm", background = "#ecfefe") %>%
  scroll_box(width = "900px", height = "250")
```

```{r echo=FALSE}
# De esta muestra selecciono únicamente las numéricas
numericasMuestra <- muestraFinal%>%select_if(is.numeric)  
#Estandarizo
NumericMuestraEstandar <- numericasMuestra %>% mutate_all (~ ( scale (.)%>% as.vector ))
# De esta muestra selecciono únicamente las categóricas
factorMuestra <- muestraFinal%>%select_if(negate(is.numeric)) 

#PCA
res.pcaMuestra <- PCA(NumericMuestraEstandar, ncp=5, graph = FALSE)
```

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
 #Gráfica para individuos 
fviz_pca_ind(res.pcaMuestra,col.ind="cos2",col.ind.sup = "black", gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```

<br/>

En el gráfico anterior se muestran los individuos de la muestra aleatoria del conjunto de datos original. Los parámetros de la función *fviz_pca_ind* permiten colorear según representatividad la contribución de los individuos. Entre las dos primeras dimensiones se recoge el 53.16% de la variabilidad de estos. Se pueden observar individuos con diferentes comportamientos. Hay aparentemente tres grupos de individuos diferenciados, en los que los datos están concentrados y se diferencian del resto. Más a la izquierda, en el segundo cuadrante, se observa una ligera dispersión de estos. Sin embargo, como ha sido previamente mencionado, gracias a la limpieza de datos esta desagrupación de observaciones es mucho menor tras eliminar outliers. Resulta interesante este primer gráfico ya que permite ver el comportamiento general de las observaciones. Sin embargo, sería de mayor utilidad poder ir analizando qué ocurre para los individuos, incluyendo también las categorías de las variables factor. 

<br/>


```{r echo=FALSE}
inputPanel(
  shiny::selectInput("categoria", label="Selección de variables", choices = factorMuestra%>%colnames(), selected = "y")
)
```

```{r echo=FALSE}
 #Gráfica para individuos segun categorías
renderPlot({
  
  muestra<-data.frame(var=pull(muestraFinal%>%select(input$categoria)),y=pull(muestraFinal%>%select(y)))
  
  fviz_pca_ind(res.pcaMuestra,
             geom.ind = "point", 
             col.ind = muestra$var, # color by groups
             addEllipses = TRUE, 
             legend.title = "Groups"
             )
  
}, height=7*75)
```

```{r echo=FALSE}
renderUI({
  HTML(paste("La variable categórica más importante que debemos analizar es la variable de salida <i>y</i>. Podemos observar que, considerada la proporción en la que aparecen estas categorías, hay una mayor proporción de individuos que contestaron <i>yes</i> que están desperdigados. En la parte superior izquierda se pueden observar individuos que probablemente sean outliers, pues están muy separados del resto de grupos. Además, se analiza también que la categoría <i>yes</i> presenta mayor variabilidad, ya que está representada por una elipse mayor. Es decir, los individuos que responden <i>no</i> están más focalizados, concentrados, pero a los que responden positivamente es más complicado asignarles unas características prefijas. Además, los individuos que responden negativamente aportan considerablemente más información a la primera dimensión que a la segunda.
             <br/><br/>
             Por otra parte, se puede analizar también la distribución de estas observaciones en función de las categorías del resto de atributos. En función del número de categorías que exista por variable será más o menos útil, pero en todas se podrán obtener algunas ideas generales. Por ejemplo, el atributo <i>job</i> apenas ofrece información. Esta variable cuenta con un gran número de clases (11 en concreto), que al formar elipses prácticamente se superponen entre ellas. Se podría destacar la categoría <i>student</i>, cuya elipse es la de mayor tamaño y que abarca prácticamente todo el plano, es decir, sus individuos son muy dispares. En lo que a <i>marital</i> refiere, ocurre algo parecido. La información obtenida no es particularmente relevante, ya que <i>divorced</i> y <i>married</i> forman una figura semejante. Además, con el resto de categorías tampoco difieren mucho, excepto con <i>unknown</i>, que sí representa una elipse diferente. Sin embargo, esta categoría poca información nos aporta, pues se trata de datos desconocidos. Reiteramos las mismas explicaciones para <i>education</i>, realmente no se distinguen intuiciones significantes. La elipse más dispar vuelve a ser aquella con la clase <i>unkown</i>, presentando mayor disparidad en los datos. Los atributos <i>default</i> y <i>contact</i>, observamos una tendencia similar. Existe una menor elipse que recoge las observaciones de una categoría más reducida, y otra que se extiende reflejando mayor variabilidad en los datos. En ambas se incluye una elipse dentro de la otra, lo que realmente no nos permite identificar en exceso patrones distintivos. En lo que a los atributos <i>housing</i> y <i>loan</i> refiere, también presentan un comportamiento prácticamente idéntico. Se tienen tres categorías, y por tanto, tres elipses, cada una de ellas dentro de la(s) otras(s). Apenas proporcionan información, todas las observaciones parecen tener un comportamiento similar y son las de <i>unkown</i> las que mayores diferencias muestran. También se pueden identificar algunas observaciones que podrían ser anómalas, que en cada caso están asociadas a distintas categorías; en <i>housing</i> la clase <i>yes</i> y en <i>loan</i> la categoría <i>no</i>. Por otra parte está la variable <i>month</i>, que en este caso sí ofrece más información. Podemos ver algunos grupos distinguidos entre sí, como lo son <i>apr</i>, <i>jul</i>, <i>aug</i>, <i>nov</i> y alguna categoría más. Se observa que <i>may</i> y <i>mar</i> son  ategorías que se entremezclan en mayor proporción con el resto, con individuos más dispersos. Finalmente es el mes de octubre (<i>oct</i>) el contiene los individuos más dispersos. En <i>day_of_week</i> las conclusiones obtenidas son insignificantes, puesto que no se observan grandes distinciones entre categorías. Por último, el atributo <i>poutcome</i> sí que presenta mayor importancia. Se distinguen categorías más diferentes entre sí y se observa que gran parte de los datos extraños son debidos a la clase <i>success</i>.
             <br/><br/>
             Todas estas intuiciones nos ayudan a entender las observaciones desde el punto de vista categórico y sus clases, y permiten identificar algunos individuos que presentan tendencias diferentes. Toda esta información obtenida servirá para contextualizar el problema y entender más profundamente las variables y la relacion entre estas.
             "))
})
```

#### {-}

#### Representación de Variables {.tabset .tabset-pills} 

##### PC1 Y PC2 

Las variables se pueden representar como puntos en el espacio de los componentes utilizando sus pesos como coordenadas. El círculo de correlaciones es un gráfico que, además de indicar el % de varianza explicada, aglutina las variables positivamente correladas en contraposición a las correladas negativamente. La calidad de representación de cada variable se mide por la longitud del vector, que numéricamente viene representado por el *cos2.* En definitiva, para estudiar la participación de cada variable en las Componentes Principales tendremos que estudiar su ángulo y longitud en conjunto.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
fviz_pca_var(res.pca5,col.var = "cos2",axes=c(1,2), gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```

<br/>

A partir del gráfico anterior vemos que, en lo que a la primera componente principal refiere, las variables que más contribuyen a esta son *euribor3m* y *emp.var.rate.*, siendo prácticamente paralelas al eje y de notable longitud. Seguida de esta se encuentra el atributo *nr.employed*, con longitud que alcanza prácticamente la unidad y que, por tanto, contribuye en gran proporción a esta componente principal. Además, presentan un ángulo muy agudo con el eje positivo de abscisas. Asímismo, *cons.price.idx* participa en menor medida en esta PC1, con una longitud menos elevada pero un ángulo significativo como para tenerlo en cuenta. Se aprecia que *previous* también participa en la construcción de la primera componente principal, a la vez que lo hace también en la segunda. No obstante, lo hace en una muy menor proproción que las anteriores mencionadas, siendo así estas primeras las más importantes. Además, influye negativamente a la construcción de la componente.  En cuanto a la variabilidad aportada, *nr.employed*, *euribor3m* y *emp.var.rate.* son las que mayor varianza proporcionan a esta primera componente. Se observa que, además, *cons.conf.idx* y *pdays* presentan correlación negativa, algo que no habíamos sospechado hasta el momento. Asimismo, se observa que *euribor3m* y *emp.var.rate.* presentan correlación positiva muy elevada, puesto que el ángulo que los separa es prácticamente inexistente. Esto ya lo habíamos analizado previamente también, junto con la variable *nr.employed*, con la que también presentan un ángulo agudo pequeño. Ahora vemos que estas tres variables también están altamente correladas con *cons.price.idx*, estando los 4 vectores muy próximos y paralelos. En términos generales, diremos que las variables más importantes para la creación de PC1 son *euribor3m*, *emp.var.rate.*, *nr.employed* y *cons.price.idx*.

En lo que a la segunda componente respecta, *pdays* es la que mayor contribución aporta a la misma, con una longitud del vector significativamente elevada (con un valor de *cos2* en torno a 0.75). A continuación se encuentran el atributo *previous*. Observamos que, en lo que a *previous* respecta, no presenta una longitud del vector elevada y forma un ángulo agudo, en torno a 45º, respecto al eje de coordenadas. Esto explica la contribución de esta variable a las dos primeras componentes principales; a PC1 negativamente y PC2 de forma positiva. Para esta segunda componente principal los vectores más importantes corresponden, por tanto, a los atributos *pdays* y *previoys*, logrando explicar el 10.9% de la varianza. La componente principal que mayor varianza explica es la primera, notablemente diferente al resto que la suceden.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
fviz_contrib(res.pca5, choice = "var", axes = 1:2, color = "skyblue", fill = "skyblue",
    linecolor = "steelblue")
```

<br/>

Como cierre, comentar que a través del histograma superior se puede observar la contribución de cada variable a estas dos primeras componentes principales. Como ha sido mencionado, *euribor3m*, *emp.var.rate.*, *nr.employed* son las que más aportan a estas dos primeras dimensiones. Prácticamente a través de estas dos primeras componentes principales se recoge toda su variabilidad. Seguida de estas se tienen los atributos *cons.price.idx* y *pdays*, que tienen también gran influencia en estas primeras dimensiones. Probablemente tengan presencia también en componentes posteriores, ya que a través de estas no se recoge su variabilidad al completo.


##### PC3 Y PC4 

Las variables se pueden representar como puntos en el espacio de los componentes utilizando sus pesos como coordenadas. El círculo de correlaciones es un gráfico que, además de indicar el % de varianza explicada, aglutina las variables positivamente correladas en contraposición a las correladas negativamente. La calidad de representación de cada variable se mide por la longitud del vector, que numéricamente viene representado por el *cos2.* En definitiva, para estudiar la participación de cada variable en las Componentes Principales tendremos que estudiar su ángulo y longitud en conjunto.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
fviz_pca_var(res.pca5,col.var = "cos2",axes=c(3,4), gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```

<br/>

En el gráfico superior podemos apreciar las distribución de variables en cuanto a la contribución de las componentes principales 3 y 4. En lo que a la tercera refiere, las variables con mayor contribución son *campaing* y *duration*. La variable *duration* presenta una longitud elevada y un ángulo agudo sobre la tercera componente, luego claramente aportará gran información a PC3. Por otra parte, *campaign* se representa mediante un vector más corto y menos paralelo. Por tanto, contribuirá a esta tercera componente pero en menor medida que *duration*. Su variabilidad se verá recogida en la tercera componente, pero también a través de otras, pues únicamente esta información no será suficiente. A través de esta tercera componente principal se puede observar correlación negativa entre los atributos *campaign* y *duration* ya que contribuyen positiva y negativamente a la componente. Esta tercera componente determina el 10.6% de la varianza total. 

Por otra parte, en lo que a la cuarta componente refiere, adquiere especial importancia el atributo *age* bien en contribución y en variabilidad. Podemos observar que *age* es el vector de mayor longitud. Claramente esto es un indicador de que la información de esta variable se recoge prácticamente al completo por la componente, en este caso cuarta, pues es un vector prácticamente paralelo al eje de coordenadas. Adquiere también una ligera y muy pequeña importancia la variable *pdays*, que, presenta corta longitud pero es ligeramente paralela al eje de coordenadas. Esta variable estaba prácticamente representada por la PC2. El porcentaje de varianza explicado por PC4 es del 9.9%.

Podemos observar que las variables más significativas en PC1 y PC2 (*euribor3m*, *emp.var.rate.*, *nr.employed* y *pdays*) han perdido, lógicamente, gran repercusión. Por el contrario, aquellas que no se explicaban en estas dos primeras componentes son las que ahora han adquirido importancia: *age* y *duration* especialmente, aunque también destaca el atributo *campaign*.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
fviz_contrib(res.pca5, choice = "var", axes = 3:4, color = "skyblue", fill = "skyblue",
    linecolor = "steelblue") 
```

<br/>

Finalmente, destacar que *duration*, *age* y *campaing* son, en especial, las que mayor información proporcionan a la creación de las dimensiones 3 y 4. El conjunto de variables se ha intentado reducir a 5 componentes principales. Por tanto, en la última que queda se deberá recoger la variabilidad faltante de las variables estudiadas y hacer especial hincapié en algunos atributos de alta repercusión y cuya información sea indispensable para el estudio.

##### PC4 Y PC5

Las variables se pueden representar como puntos en el espacio de los componentes utilizando sus pesos como coordenadas. El círculo de correlaciones es un gráfico que, además de indicar el % de varianza explicada, aglutina las variables positivamente correladas en contraposición a las correladas negativamente. La calidad de representación de cada variable se mide por la longitud del vector, que numéricamente viene representado por el *cos2.* En definitiva, para estudiar la participación de cada variable en las Componentes Principales tendremos que estudiar su ángulo y longitud en conjunto.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
par(mfrow=c(1,2))
fviz_pca_var(res.pca5,col.var = "cos2",axes=c(4,5), gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```

<br/>

A través del gráfico superior se analizan las componentes principales 4 y 5. La cuarta, recién explicada, representa profundamente las variables *duration* y *campaign*. En lo que a la quinta y última componente refiere, se observa que el atributo que mayor contribución aporta es *age*. De hecho, el vector *age* es el más largo, indicador de la importancia del mismo en estas componentes. Cierto es que parte de su información ha sido explicada por PC3, pero PC5 también recoge gran parte de su variabilidad e información. 

Cabe destacar que en estas últimas componentes principales se aprecia que gran parte de la contribución y variabilidad de los atributos ya había sido explicada, por lo que se obtienen vectores más cortos y menos paralelos. Por ejemplo, se aprecia que un gran número de vectores se aglutinan en el centro del gráfico, lo que indica contribuyen a componentes principales previas. Esto se debe a que la mayor parte de la varianza se trata de concentrar en las primeras componentes principales, luego a medida que se van añadiendo estas, las variables van contribuyendo en menor medida.

<br/>

```{r fig.align="center",echo=FALSE, warning=FALSE}
fviz_contrib(res.pca5, choice = "var", axes = 4:5, color = "skyblue", fill = "skyblue",
    linecolor = "steelblue") 
```

<br/>

A través del gráfico superior se da un cierre a la sección de componentes principales. Se observa que, a medida que se van añadiendo estas, la aportación de las variables es cada vez menor, pues se trata de recoger toda la información en las primeras componentes. En este caso, entre las dimensiones 4 y 5 se recoge la variabilidad especialmente de *age*, además de las variables *campaign* y *duration*, pero en mucha menor medida, pues ya había sido explicada por componentes anteriores. Además, se observa cómo hay atributos que aportan muy poca información, ya que han sido explicados con anterioridad.

#### {-}

### {-}

***

<br/>

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Conclusiones de la técnica</a>

Como ha sido mencionado, el análisis PCA pretende reducir el conjunto inicial de variables conservando siempre la máxima información y variabilidad posibles. Para ello, lo que genera son Componentes Principales que van adquiriendo la información de las variables del conjunto original de datos, de tal manera que las Componentes Principales podrían sustituir a estos atributos presentando prácticamente la misma funcionalidad.

En suma, las cinco Componentes Principales seleccionadas podrían sustituir al conjunto de datos inicial y proporcionar información muy similar. De esta manera, se reduce el coste computacional del estudio y los resultados finales serían muy parecidos a los obtenidos con el dataset original.

> El análisis PCA solo es aplicable a variables numéricas. Sin embargo, al igual que hemos hecho anteriormente, sería posible codificar los atributos categóricos numéricamente y aplicarles este análisis. No obstante, si el número de clases posibles de una variable cualitativa es muy grande podría dar lugar a muchas variables. Conviene entonces ver, a través de otras técnicas, si podemos agrupar las clases o categorías para evitar tener variables que casi siempre toman el mismo valor.

Además, se han reafirmado las suposiciones realizadas a través de la matriz de correlación. Las variables *nr.employed*, *euribor3m*, *emp.var.rate.* y *cons.conf.idx* están relacionadas y aportan información a la PC1. Asimismo, *pdays* y *previous* también lo están entre ellas, contribuyendo a la PC2 y presentando ligeras correlaciones negativas con los primeros cuatro atributos.

<br/>


## ANÁLISIS FACTORIAL

<br/>

El Análisis Factorial (AF) es un método de reducción estadística que tiene por objeto explicar un conjunto  de variables observadas por un pequeño grupo de variables latentes, o no observadas, denominadas factores. Está relacionado con los componentes principales, pero trata de analizar las covarianzas en vez de explicar la varianza y presupone un modelo estadístico formal que genera la muestra en vez de ser una herramienta descriptiva.

Dentro del AF se distinguen el análisis factorial exploratorio, que supone que cualquier indicador o variable puede estar asociado a cualquier factor, y el análisis factorial confirmatorio, que se utiliza para determinar el factor y la carga factorial de las variables medidas, y para confirmar lo que se espera de la teoría preestablecida. No obstante, no realizaremos ninguna distinción en la aplicación del método.

<br/>
<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Objetivo de la técnica</a>
<br/>

La finalidad de la aplicación de técnica es encontrar el conjunto subyacente de datos que sea capaz de explicar las variables numéricas de nuestro conjunto de datos. Más concretamente, trataremos de sustituir los unos de la diagonal de la matriz de correlaciones por estimaciones de la varianza que cada variable tiene en común con las demás. Cuanto mayores sean las estimaciones de esta diagonal, denominadas como comunalidades, una mayor proporción de varianza será explicada por los factores.

Aplicamos el análisis factorial de acuerdo a los pasos siguientes.

<br/>

### Adecuación de la muestra

***

Para comprobar si las características de los datos son las adecuadas para realizar un Análisis Factorial analizamos la matriz de correlaciones muestrales, ya que uno de los requisitos que deben cumplirse es que las variables se encuentren altamente intercorrelacionadas.

Existen varios indicadores para analizar la matriz de correlación, sin embargo utilizaremos el índice KMO debido a que es la medida más extendida, es robusta y se basa en el análisis de correlaciones parciales. También emplearemos el test de esfericidad de Barlett , test que compara una matriz de correlación observada con la matriz identidad y que asume que las variables se ajustan a una distribución normal multivariante, con el objetivo de reforzar el resultado del índice KMO.

<br/>
```{r, echo=FALSE, fig.align='center'}
ggcorrplot(cor(numerics), type = 'lower', title = 'Matriz de correlaciones',show.diag = FALSE, hc.order = TRUE, lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```
<br/>

La matriz de correlaciones obtenida tras eliminar las instancias consideradas como outliers difiere ligeramente de la matriz anterior.

Ahora apreciamos con mayor claridad la existencia de correlaciones positivas entre las variables *euribor3m*, *emp.var.rate*, *nr.employed*, *cons.price.idx* y *cons.conf.idx*. Por otro lado, vemos cómo las correlaciones negativas que tenía la variable *previous* con el resto de variables se atenúan. Cabe remarcar que los pares de variables entre los que no se percibía correlación se mantienen en la misma posición.

Asimismo, unas primeras conlcusiones de lo anterior es que las variables *duration*, *pdays*, *age* y *campaign* no aportarán información adecuada al modelo factorial debido a que la correlación entre estas y el resto de variables es próxima a cero.

<br/>
```{r echo=FALSE}
cortest.bartlett(cor(numerics), n = nrow(numerics))
KMO(cor(numerics))
```
<br/>

De acuerdo al valor del índice KMO la muestra es aceptable. Esto sugiere que las variables comparten factores comunes y por tanto el coeficiente de correlación parcial entre pares de variables es relativamente bajo. No obstante pensamos que si se prescindiera de determinadas variables el valor del índice aumentaría.

Las variables consideradas redundantes serán aquellas cuyo MSA es menor. Por tanto, las primeras impresiones nos dicen que podríamos prescindir de las variables *cons.conf.idx* y *duration*. Analizaremos este aspecto al generalizar los datos en la validación del modelo, una vez dispongamos de más información. 

Por otro lado, el valor del test de Barlett reafirma que las variables están correlacionadas, ya que el p-value no es significativo y por tanto los datos son adecuados para el análisis factorial. Gracias a estos resultados podemos proseguir con las siguientes fases. En caso de haber obtenido un resultado desfavorable deberíamos plantearnos la utilización de otros métodos de interdependencia.

<br/>

### Número de factores

***

Como la matriz factorial puede representar un número de factores superior al necesario para explicar la estructura de los datos originales, tendremos en cuenta algunos criterios para determinar el número de factores a conservar.

Este es un aspecto complicado del análisis factorial, ya que si tuviéramos una hipótesis sobre las variables latentes, podríamos comenzar con una suposición. Sin embargo, al no tener ninguna pista, utilizamos el análisis  de componentes principales para obtener una buena estimación inicial del número de factores. Es decir, consideramos que el número de factores del que debemos partir es 5.

Cabe mencionar que entre los métodos para extraer el factor del conjunto de datos, el análisis de componentes principales es el más utilizado por los investigadores. No obstante, como mencionábamos anteriormente, la decisión del número de componentes a tomar depende exclusivamente del investigdor, por lo que tener en cuenta otras reglas y criterios como la regla de Kaiser o el criterio de determinación a priori sería adecuado.

Teniendo en cuenta lo mencionado, se genera un modelo acorde a estas características y se muestra una primera descripción. Más adelante profundizaremos en los resultados.

<br/>
```{r, echo=FALSE}
#Aumentamos el límite inferior para las unicidades durante la optimización, de modo que permita la convergencia de una solución

res.factanal <- factanal(numerics, factors = 5, scores = "regression", rotation = "varimax", lower = 0.015)
res.factanal
```
<br/>

### Interpretación {.tabset .tabset-pills} 

***

Para estudiar los factores es necesario conocer el comportamiento de estos. Por consiguiente, analizamos las salidas que proporciona el modelo.

La primera salida, que proporciona las unicidades, hace referencia a la proporción de variabilidad que no puede explicarse mediante una combinación lineal de los factores. Una alta unicidad de una variable indica que los factores no son capaces de explicar su varianza.

<br/>
```{r, echo=FALSE}
res.factanal$uniquenesses
```
<br/>

En nuestro modelo observamos cómo las unicidades de las varibles *age*, *duration* y *pdays* toman valores cercanos a uno. Es decir, los factores no son capaces de explicar su varianza, hecho que favorece la hipótesis planteada al observar la matriz de correlaciones. Asimismo, obtendríamos las comunalidades restando las unicidades a uno, pero proporcionan la misma información que las unicidades, ya que su valor es la fracción de la varianza total de la variable explicada por el factor.

Continuamos con los pesos, que indican la contribución de cada variable original al factor. Las variables con una carga alta están bien explicadas por el factor, mientras que ocurre lo contrario cuando no tienen valor. Por lo tanto, identificamos las variables cuyas correlaciones con el factor son más elevadas en valor absoluto.

<br/>
```{r, echo=FALSE, fig.align='center'}
res.factanal$loadings
corrplot(res.factanal$loadings, is.corr=FALSE, col = c("#c75fe7","#f7e9fc" ,"#88edeb"), tl.col = "darkblue")
```
<br/>

Observamos que la contribución de las variables *emp.var.rate*, *euribor* y *nr.employed* al primer factor es muy grande. Hecho que no sorprende debido a la alta correlación que mostraban entre ellas y que se había mencionado con anterioridad.

Por otro lado, vemos que en los tres siguientes factores es una única variable la que contribuye a su construcción. Finalmente, podríamos consideramos que el quinto factor no es necesario, ya que la variable *cons.conf.idx* contribuye en mayor proporción a la creación del primer factor.

De acuerdo a otras reglas y criterios como la regla de Kaiser, criterio que hace alusión al Análisis de Componentes Principales, tomaríamos como número de factores el número de valores propios superiores a 0'7 para determinar el número de factores a conservar. Es decir, consideraríamos que el quinto factor sería prescindible. No obstante, no lo eliminamos ya que un análisis externo muestra que los valores de las unicidades para un modelo construido con cuatro factores se aproximan más a uno y por tanto los factores no son capaces de explicar la variabilidad de un grupo mayor de variables.

Por último mostramos visualmente los resultados obtenidos a través de gráficos con el objetivo de favorecer la interpretación.

<br/>

#### Representación de individuos


Como el conjunto de datos está compuesto por un gran número de observaciones, realizar un gráfico con todos ellos sería confuso, bien por coste computacional y bien por visibilidad, ya que al representar tantas observaciones se amontonarían entre ellas y se acabaría obteniendo una nube imposible de interpretar. Por este motivo, seleccionamos la misma muestra con la que trabajamos al representar los individuos en componentes principales.

```{r echo=FALSE}
conjuntoNo <- sinOutliers %>% filter(y=="no") 
conjuntoYes <- sinOutliers %>% filter(y=="yes") 
n = 500

muestraNo<- conjuntoNo %>% sample_n(size=round(0.8873458*n, 0), replace = FALSE) 
muestraYes<- conjuntoYes %>% sample_n(size=round(0.1126542*n, 0), replace = TRUE)

muestraFinal <- rbind(muestraNo, muestraYes)

# De esta muestra selecciono únicamente las numéricas
numericasMuestra <- muestraFinal%>%select_if(is.numeric)  

#AF
fac.muestra <- factanal(numericasMuestra, factors = 5, scores = "regression", rotation = "varimax", lower = 0.015)
```

<br/>
```{r fig.align="center",echo=FALSE, warning=FALSE}
biplot(fac.muestra$scores[, 1:2], loadings(fac.muestra)) 
```
<br/>

En el gráfico anterior se observan agrupaciones de individuos prómiximas al centro de los datos. Esto quiere decir, entre otras cosas, que son observaciones que no están bien representadas por los dos primeros factores. Por otro lado, los individuos que se hallan alejados del centro quedarán bien resumidos por estos factores.

Hemos de tener en cuenta que se trata de una submuestra del conjunto de datos, por lo que nuestras interpretaciones son meramente informativas y tratan de enseñar cómo obtener conclusiones.

#### Representación de Variables {.tabset .tabset-pills}


##### F1 Y F2 

La representación se hace tomando los factores dos a dos, donde cada factor representa un eje de coordenadas. Sobre estos ejes se proyectan las variables originales, que se pueden reproducir como puntos en el espacio utilizando los coeficientes de correlación entre la variable y el factor como coordenadas??. Las variables cerca del origen tienen correlaciones reducidas en ambos factores, mientras que las variables al final de un eje tienen correlaciones altas sólo con ese factor, y en consecuencia lo describen. En definitiva, para estudiar la participación de cada variable en los factores tendremos que estudiar su ángulo y longitud en conjunto, al igual que ocurría en PCA.

<br/>
```{r, echo=FALSE, fig.align='center'}
a <- loadings(res.factanal)
Cargas <- data.frame(a[,c(1,2)])
Cargas$Variables <- row.names(Cargas)
ggplot(Cargas) + xlim(-1, 1.4) + ylim(-1, 1) + geom_text(aes(x = Factor1, y = Factor2, label = row.names(Cargas)),
    hjust = -0.1, vjust = 0, ) + geom_hline(yintercept = 0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) +
    ggtitle("Factores y variables") + geom_segment(aes(x = 0, y = 0, xend = Factor1,
    yend = Factor2), arrow = arrow(angle = 20), size = 0.2, color = "steelblue") + 
    theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour="#FFFFFF"))
```
<br/>

De acuerdo al gráfico obtenido, vemos cómo los vectores de las variables *euribor3*, *emp.var.rate*, *cons.price.idx* y *nr.employed* son de gran dimensión y forman ángulos agudos entre ellos. Esto resalta lo mencionado anteriormente: contribuyen al factor 1 ,debido a que el ángulo que forman con el eje de abscisas es agudo, y están altamente correlacionados. Asimismo, sería un acierto interpretar que el primer factor representa al conjunto de variables económicas y sociales del que se venía hablando.

Por otro lado, la variable *cons.conf.idx* queda bien representada por el segundo factor. La variabilidad del resto de variables no queda recogida por los dos primeros factores.


##### F3 Y F4 

La representación se hace tomando los factores dos a dos, donde cada factor representa un eje de coordenadas. Sobre estos ejes se proyectan las variables originales, que se pueden reproducir como puntos en el espacio utilizando los coeficientes de correlación entre la variable y el factor como coordenadas. Las variables cerca del origen tienen correlaciones reducidas en ambos factores, mientras que las variables al final de un eje tienen correlaciones altas sólo con ese factor, y en consecuencia lo describen. En definitiva, para estudiar la participación de cada variable en los factores tendremos que estudiar su ángulo y longitud en conjunto, al igual que ocurría en PCA.

<br/>
```{r, echo=FALSE, fig.align='center'}
a <- loadings(res.factanal)
Cargas <- data.frame(a[,c(3,4)])
Cargas$Variables <- row.names(Cargas)
ggplot(Cargas)  +
    xlim(-1, 1.4) + ylim(-1, 1) + geom_text(aes(x = Factor3, y = Factor4, label = row.names(Cargas)),
    hjust = -0.1, vjust = 0, ) + geom_hline(yintercept = 0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) +
    ggtitle("Factores y variables") + geom_segment(aes(x = 0, y = 0, xend = Factor3,
    yend = Factor4), arrow = arrow(angle = 20), size = 0.2, color = "steelblue") + 
    theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour="#FFFFFF"))
```
<br/>

Observando el gráfico demostramamos de manera visual las concluisones anteriores. Únicamente las variables *previous* y *campaign* contribuyen a la creación del tercer y cuarto factor, cada una por separado. 

##### F4 Y F5

La representación se hace tomando los factores dos a dos, donde cada factor representa un eje de coordenadas. Sobre estos ejes se proyectan las variables originales, que se pueden reproducir como puntos en el espacio utilizando los coeficientes de correlación entre la variable y el factor como coordenadas. Las variables cerca del origen tienen correlaciones reducidas en ambos factores, mientras que las variables al final de un eje tienen correlaciones altas sólo con ese factor, y en consecuencia lo describen. En definitiva, para estudiar la participación de cada variable en los factores tendremos que estudiar su ángulo y longitud en conjunto, al igual que ocurría en PCA.

<br/>
```{r, echo=FALSE, fig.align='center'}
a <- loadings(res.factanal)
Cargas <- data.frame(a[,c(4,5)])
Cargas$Variables <- row.names(Cargas)
ggplot(Cargas) +
    xlim(-1, 1.4) + ylim(-1, 1) + geom_text(aes(x = Factor4, y = Factor5, label = row.names(Cargas)),
    hjust = -0.1, vjust = 0, ) + geom_hline(yintercept = 0, linetype = 2) + geom_vline(xintercept = 0, linetype = 2) +
    ggtitle("Factores y variables") + geom_segment(aes(x = 0, y = 0, xend = Factor4,
    yend = Factor5), arrow = arrow(angle = 20),size = 0.2, color = "steelblue") + 
    theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour="#FFFFFF"))
```
<br/>

A partir de la representación anterior vemos que la influencia de la variable *cons.price.idx* en la obtención del quinto factor es menor en proporción que la influencia en el primer factor. En el supuesto de graficar conjuntamente el primer y quinto factor, esperaríamos que el vector que representa dicha variable formase un ángulo de 45º con ambos ejes.

#### {-}

### {-}
<br/>

### Validación del modelo {.tabset .tabset-pills}

***

El último paso en el AF es estudiar la validez del modelo. Este proceso debe realizarse analizando la bondad de ajuste y la generalidad de los datos.

Por un lado, debido a la suposición de que la correlacion observada entre las variables puede atribuirse a factores comunes, pueden estudiarse las diferencias entre las correlaciones observadas y las correlaciones reproducidas. Así, a partir de la matriz residual, concluiremos que el modelo estimado se ajusta a los datos cuando los resiudos sean pequeños.

<br/>
```{r, echo=FALSE, fig.align='center', fig.dim=c(14,8)}
# Matriz residual
Lambda <- res.factanal$loadings
Psi <- diag(res.factanal$uniquenesses)
S <- res.factanal$correlation
Sigma <- Lambda %*% t(Lambda) + Psi

a<-ggcorrplot(S, type = 'lower', title = 'Matriz de correlaciones',show.diag = FALSE, hc.order = TRUE, lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))

b<-ggcorrplot(round(S - Sigma, 6), type = 'lower', title = 'Matriz residual',show.diag = FALSE, hc.order = TRUE, lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))

(a|b)

```
<br/>

Como los valores de la matriz residual son cercanos o iguales a cero, consideramos que nuestro modelo factorial es una buena representación del concepto subyacente.

Por otro lado, es conveniente realizar nuevos análisis factoriales modificando las variables consideradas, bien sea eliminando aquellas variables que no tienen relación con ningún factor o eliminando las variables con relaciones más fuertes tratando de descubrir cómo se comporta el resto de ellas sin su presencia.

Consideramos así un nuevo modelo que no incluya las variables *age*, *duration* y *pdays*, ya que los factores no eran capaces de explicar su varianza, y realizamos un análisi factorial siguiendo los pasos anteriores.

<br/>

#### Adecuación de la muestra

<br/>
```{r, echo=FALSE, fig.align='center'}
ggcorrplot(cor(numerics[,-c(1,2,4)]), type = 'lower', title = 'Matriz de correlaciones',show.diag = FALSE, hc.order = TRUE, lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))
```
<br/>

En el gráfico anterior obtenemos la matriz de correlaciones resultante al eliminar las variables consideradas redundantes o no explicadas por el modelo más general.

Los valores que se presentan son idénticos a los obtenidos con anterioridad, por lo que las conclusiones son las mismas.

<br/>
```{r echo=FALSE}
cortest.bartlett(cor(numerics[,-c(1,2,4)]), n = nrow(numerics))
KMO(cor(numerics[,-c(1,2,4)]))
```
<br/>

De acuerdo a los diferentes métodos para estudiar la adecuación del modelo, se considera que el modelo recoge de manera adecuada la variabilidad de los datos, no obstante no mejora el índice KMO respecto del modelo que incluye todas las variables.

<br/>

#### Número de factores


Debido a que el número de variables consideradas se ha visto reducido, consideramos que el número de factores en el modelo debe disminuir.

Así, y teniendo en cuenta que existe una restricción sobre el número máximo de factores de acuerdo a la ecuación $(p-m)^2 > p + m$ , donde *p* representa al número de variables iniciales y *m* al número de factores a considerar, interpretamos que un número de factores óptimo con el que comenzar sería de tres.

Mostramos una primera descripción del modelo.

<br/>
```{r, echo=FALSE}
res2.factanal <- factanal(numerics[,-c(1,2,4)], factors = 3, scores = "regression", rotation = "varimax", lower = 0.05)
res2.factanal
```
<br/>

#### Interpretación

Comenzamos el análisis observando las unicidades del modelo.


```{r echo=FALSE}
res2.factanal$uniquenesses
```
<br/>

La tabla anterior sugiere que los factores no son capaces de explicar la variabilidad de las variables *campaign* y *previous*. No obstante, comparando las unicidades del resto de variables nos damos cuenta de que se han visto reducidas. Es decir, los factores recogen la información mejor para estas variables.

Proseguimos analizando los pesos, que indican, como se ha comentado perviamente, la contribución de cada variable original al factor.


<br/>
```{r, echo=FALSE, fig.align='center'}
res2.factanal$loadings
corrplot(res2.factanal$loadings, is.corr=FALSE, col = c("#c75fe7","#f7e9fc" ,"#88edeb"), tl.col = "darkblue")
```
<br/>

A partir del gráfico anterior se recoge la información que se obtenía teniendo en cuenta el modelo con todas las variables. Sin embargo, de acuerdo a la regla de kaiser, ahora el número de factores a considerar es el adecuado.

Finalizamos observando la contribución de las variables a la creación de los dos primeros factores y los interpretamos de igual manera a como se ha hecho con anterioridad.

<br/>
```{r, echo=FALSE, fig.align='center'}
a <- loadings(res2.factanal)
Cargas <- data.frame(a[,c(1,2)])
Cargas$Variables <- row.names(Cargas)
ggplot(Cargas) +
    xlim(-1, 1.4) + ylim(-1, 1) + geom_text(aes(x = Factor1, y = Factor2, label = row.names(Cargas)),
    hjust = -0.1, vjust = 0, ) + geom_hline(yintercept = 0, linetype = 2) + geom_vline(xintercept = 0,linetype = 2) +
    ggtitle("Factores y variables") + geom_segment(aes(x = 0, y = 0, xend = Factor1,
    yend = Factor2), arrow = arrow(angle =20),size = 0.2, color = "darkblue") + 
    theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour="#FFFFFF"))
```
<br/>

Podemos ver nuevamente cómo las variables *euribor3m*, *emp.var.rate* y *nr.employed* quedan bien explicadas por el primer factor, mientras que la variable *cons.conf.idx* quda representada por el segundo factor. Asimismo, respecto a los gráficos anteriores, ahora las variables *previous* y *cons.price.idx* ven afectada la contribución a los dos factores.

Obtenemos las mismas conclusiones anteriores. El primer factor representa al conjunto de variables económicas y sociales.


#### Validación del modelo


El último paso en el AF es estudiar la validez del modelo. Este proceso debe realizarse analizando la bondad de ajuste y la generalidad de los datos.

Por un lado, debido a la suposición de que la correlacion observada entre las variables puede atribuirse a factores comunes, pueden estudiarse las diferencias entre las correlaciones observadas y las correlaciones reproducidas. Así, a partir de la matriz residual, concluiremos que el modelo estimado se ajusta a los datos cuando los resiudos sean pequeños.

<br/>
```{r, echo=FALSE, fig.dim=c(14,8)}
# Matriz residual
Lambda <- res2.factanal$loadings
Psi <- diag(res2.factanal$uniquenesses)
S <- res2.factanal$correlation
Sigma <- Lambda %*% t(Lambda) + Psi


# Gráficos de correlaciones
a<-ggcorrplot(S, type = 'lower', title = 'Matriz de correlaciones',show.diag = FALSE, hc.order = TRUE, lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))

b<-ggcorrplot(round(S - Sigma, 6), type = 'lower', title = 'Matriz residual',show.diag = FALSE, hc.order = TRUE, lab = TRUE, colors = c("#c75fe7","#f7e9fc" ,"#88edeb"))
(a|b)
```
<br/>

De acuerdo con la matriz residual los resultados son ligeramente peores respecto de los obtenidos con el modelo que tenía en cuenta todas las variables. Por esta razón y porque el ínidce KMO se mantiene en un valor de 0'68, consideramos que no es beneficioso eliminar las variables previamente propuestas.

### {-}

***

<br/>

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Conclusiones de la técnica</a>

Los resultados obtenidos a lo largo del análisis proporcionan información muy similar a las conclusiones obtenidas con el análisis de componentes principales. De hecho, se considera el cinco como valor óptimo para el número de factores. De esta manera, se reduce el coste computacional del estudio y más importante, la interpretación de los datos se facilita. 

<br/>

## ANÁLISIS DE CORRESPONDENCIAS

<br/>

El análisis de Correspondencia (CA) es una técnica estadística que analiza, desde un punto de vista gráfico, las relaciones de dependencia e independencia de un conjunto de variables categóricas. Este método extrae información a partir de los datos de una tabla de contingencia y trata de resumir la información presente en filas y columnas de manera que pueda proyectarse sobre un subespacio reducido y que represente simultáneamente los puntos fila y los puntos columa.

En esencia, es un tipo especial de análisis de componentes principales, pero realizado sobre una tabla de contingencia y usando una distancia euclídea ponderada llamada chi-cuadrado. Por esta razón la interpretación de algunos gráficos se hace de igual manera que con PCA.


<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Objetivos de la técnica</a>

Los objetivos básicos que nos proponemos son medir la asociación de solo una fila o columna, para ver si las modalidades de una variable pueden ser combinadas y estudiar si existe relación entre las categorías de las filas y columnas.

Con este fin seleccionamos dos de las variables categóricas que consideramos de gran interés y que, a su vez, tienen más de dos categorías, ya que sino la representación en los gráficos es pobre. Debemos tener en cuenta de que el Análisis de Correspondencias Simple estudia los pares de variables dos a dos, por lo que algunas conclusiones se podrían ver afectadas al estudiar el total de categorías a través del análisis de correspondencias múltiples.

<br/>

#### Planteamiento del problema

***

Como bien hemos mencionado, el análisis dependerá de las variables que escojamos representar. Suelen seleccionarse pares de variables donde una de ellas represente individuos y la otra represente cualidades. De esta manera, consideramos que las variables *job* y *marital* son las adecuadas para el estudio debido a las conclusiones obtenidas en análisis univariante.

Comenzamos observando a través de un gráfico las frecuencias de este problema bidimensional. Dicho de otra manera, 
representamos la tabla de contingenia de los datos y vemos cómo es la distribución de cada variable.

<br/>
```{r echo=FALSE, fig.align='center', fig.dim= c(14,6)}
tabla <- data.frame(factors %>% group_by(job) %>% count(marital))
tabla <- tabla %>% spread(key = marital, value = "n") %>% mutate_all( ~replace(., is.na(.), 0))
rownames(tabla) <- pull(tabla %>% select(job))
tabla <- tabla %>% select(-job)


balloonplot(as.table(as.matrix(tabla)), main ="Tabla de Contingencia", xlab ="", ylab="",
        label = FALSE, show.margins = FALSE)
```
<br/>

A partir del gráfico anterior y fijándonos en las intersecciones de cada fila con cada columna, podría decirse que hay alguna combinación de categorías que tienen una frecuencia mucho mayor que otras. Esto ocurre, por ejemplo, con las categorías *married* y *blue-collar*, donde hay un gran número de individuos casados trabajando en la construcción comparado con las personas casadas que se encuentran estudiando.

<br/>

#### Dependencia e independencia

***

Para comprobar la existencia de algún tipo de relación entre las variables tendremos en cuenta el test de hipótesis Chi-cuadrado de Pearson, donde se contrasta la hipótesis nula que presupone la independencia entre ambas variables. El test se basa en comparar los perfiles fila y columna con los perfiles marginales correspondientes considerando que si la hipótesis nula es cierta, todos los perfiles fila son iguales entre sí e iguales al perfil marginal de la variable representada en filas. Aplicamos el test correspondiente.

<br/>
```{r echo=FALSE, warning=FALSE, message=FALSE}
chisq <- chisq.test(tabla)
chisq
```
<br/>

Observamos que el p-valor del test toma un valor inferior a 0'05, lo que conlleva a que se rechaze la independencia entre variables. Es decir, las variables tienen dependencia y por tanto la inercia, media de las distancias al cuadrado de cada punto al centro de gravedad, toma valores bajos indicando las similitudes entre datos.


<br/>

#### Valores y vectores propios

***

Al igual que en el caso de componentes principales analizamos los valores propios, que identifican a la cantidad de inercia explicada por cada eje, para determinar el número de ejes o dimensiones a considerar. En esta ocasión no utilizaremos ninguna regla sino que mantendremos las dimensiones de acuerdo a la proporción de varianza explicada.

<br/>
```{r, echo=FALSE, fig.align='center'}
res.ca <- CA(tabla, graph = FALSE)
fviz_screeplot(res.ca, addlabels=TRUE, barcolor = "skyblue", barfill = "skyblue",
    linecolor = "steelblue")
```
<br/>

En nuestro caso, observando el gráfico anterior, consideramos que es correcto conservar una dimensión o dos dimensiones, ya que la proporción total de varianza explicada sería superior al 80% y 95% respectivamente. También podríamos obtener concluiones a partir de la regla del codo, pero el resultado es el mismo.

#### Interpretación {.tabset .tabset-pills}

***

##### Representación de Filas

Mostramos viasualmente los puntos de fila y los coloreamos de acuerdo con su calidad en el mapa de factores. La calidad de representación será simplemente la suma del coseno al cuadrado, por lo que valores cercanos a uno significarán una buena representación. Asimismo, añadimos un gráfico de barras que muestra la contribución de cada variable fila a la primera dimensión.

<br/>
```{r echo=FALSE, fig.align='center', fig.dim=c(16,8)}
a <- fviz_ca_row(res.ca, col.row = "cos2",
         gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"), 
         repel = TRUE)

b<- fviz_contrib(res.ca, choice = "row", axes = 1, top = 10) +  scale_fill_manual(paletaContinua)

(a|b)

```
<br/>

En el primer gráfico observamos cómo todas las categorías, excepto *unknown*, *unemployed* y *self-employed*, están muy bien representadas por las dos primeras dimensiones. Además, el hecho de que existan variables cuya representación no es adecuada no nos preocupa excesivamente, ya que conocemos sus características y su influencia en el conjunto de datos.

Por otro lado, de acuerdo con el segundo gráfico vemos cómo las variables *student*, *admin* y *blue-collar* contribuyen en gran medida a la primera dimensión.

##### Representación de Columnas

En los gráficos siguientes se representan las variables columna de acuerdo con su calidad en el mapa de factores. Hemos de tener en cuenta que exiten menos categorías para esta variable, lo que supone una visualización más pobre.

<br/>
```{r echo=FALSE, warning=FALSE, fig.dim=c(16,8)}
a <- fviz_cos2(res.ca, choice = "col", axes = 1:2, color = "skyblue")

b<- fviz_ca_col(res.ca, col.col = "contrib",
         gradient.cols = c("#c75fe7","#f7e9fc" ,"#88edeb"), 
         repel = TRUE)

(a|b)
```
<br/>

De acuerdo con el primer gráfico, todas las categorías, excepto *unknown*, están muy bien representadas por las dos primeras dimensiones. Esto resalta el hecho de que la categoría *unknown*, al tratarse de datos no facilitados, no es tenida en cuenta por las dimensiones.

El segundo gráfico, a su vez, muestra cómo las variables que más contribuyen a las dos dimensiones son *single* y *married*. Este resultado no nos sorprende, ya que se habían recogido muchos más datos con estas respuestas. Finalmente mencionamos lo obvio, las variables *married* y *single* se encuentran en cuadrantes opuestos debido a su correlación negativa.

<br/>

#### Representación Biplot

Los gráficos biplot nos permiten visualizar las variables fila y columna de forma conjunta y observar similitudes y diferencias entre ellas. La distancia entre cualquier punto de fila o columna da una medida de su similitud. No obstante, solo se puede interpretar realmente la distancia entre puntos de fila o la distancia entre puntos de columna. Por ello, para interpretar la distancia entre los puntos de columna y fila, presentamos los perfiles columna en espacio de fila. Este tipo de mapa se llama biplot asimétrico y para construirlo proyectamos los puntos de filas a partir de las coordenadas estándar (S) y los perfiles de las columnas a partir de las coordenadas principales (P).

<br/>
```{r, echo=FALSE, fig.dim=c(16,8)}
a<- fviz_ca_biplot(res.ca, repel = TRUE)

b<- fviz_ca_biplot(res.ca, 
           map ="colprincipal", arrow = c(TRUE, TRUE),
           repel = TRUE) 
(a|b)
```
<br/>

Observando el primer gráfico podemos mencionar que dependiendo del estado civil del usuario contactado el trabajo al que se dedica es uno u otro. Por ejemplo, vemos cómo todos los trabajos excepto *unknown*, *single* y *retired* se encuentran agrupados en torno a las categorías *married* y *single*. Sin embargo, como hemos mencionado anteriormente, no podemos realizar estas interpretaciones. Por esta razón analizamos el segundo gráfico. No obstante, como las variables columna se agrupan alrededor del origen de coordenadas no se obtiene ninguna conclusión complementaria.

Por otro lado, debido a la dificultad de conocer los puntos que más contribuyen a la solución de la CA, se muestra a continuación el biplot de contribución. En esta pantalla, los puntos que contribuyen muy poco a la solución están cerca del centro de la biplot y son relativamente poco importantes para la interpretación.

En nuestro caso interpretaremos la contribución de las filas a los ejes. Cabe recordar que las columnas están en coordenadas principales y las filas en coordenas estándar multiplicadas por la raíz cuadrada de la masa (peso proporcional a su importancia en el conjunto).

En el gráfico siguiente se resume la información proporcionada tanto por las representación por columnas como por filas. 

<br/>
```{r echo=FALSE, fig.align='center'}
fviz_ca_biplot(res.ca, map ="colgreen", arrow = c(TRUE, FALSE),
           repel = TRUE)

```
<br/>


#### {-}

<br/>

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Conclusiones de la técnica</a>

El análisis de correspondencias ha resultado muy útil para identificar las relaciones entre categorías de las dos variables cualitativas que considerábamos más importantes. Por otro lado, hubise sido adecuado realizar un análiis múltiple que tuviera en cuenta todas las variables y las relaciones entre ellas. 

<br/>

## ANÁLISIS CLUSTER

El análisis cluster es un método estadístico que permite clasificar los individuos de un conjunto logrando que las características de los integrantes sean lo más parecidas posible. Es decir, el objetivo de esta técnica es agrupar los elementos en grupos homogéneos en función de las similitudes entre ellos.

Para la realización de esta técnica es necesaria la definición de un criterio para medir la semejanza entre los individuos.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Distancias a utilizar</a>

Con el objetivo de dar un valor numérico a la relación entre individuos, vamos hacer uso  de distintas distancias, a continuación, se exponen las distancias que se tomarán como referencia en el estudio.

<a style="text-decoration: 
none; border-bottom: 
1px solid #000000; 
color: #000000;">Manhatam</a>

La distancia de *Manhatam* es una medida de distancia donde los grupos formados contendrán individuos parecidos de forma que la distancia entre ellos tiene que ser pequeña. Esta medida define la distancia entre dos puntos p y q como el sumatorio de las diferencias absolutas entre cada dimensión. Esta medida se ve menos afectada por outliers (es más robusta) que la distancia euclídea debido a que no eleva al cuadrado las diferencias. La distancia de *Manhatam* se define como:

$$d_{\operatorname{man}}(p, q)=\sum_{i=1}^{n}\left|\left(p_{i}-q_{i}\right)\right|$$
<a style="text-decoration: 
none; border-bottom: 
1px solid #000000; 
color: #000000;">Distancia de Gower para datos mixtos</a>

A la hora de tener variables numéricas y categóricas, como es nuestro caso, la distancia de Gower cuantifica las diferencias entre estos tipos de datos siguiendo la siguiente definición  $$d_{i j}^{2}=1-s_{i j}$$, donde: 


$$s_{i j}=\frac{\sum_{h=1}^{p_{1}}\left(1-\left|x_{i h}-x_{j h}\right| / G_{h}\right)+a+\alpha}{p_{1}+\left(p_{2}-d\right)+p_{3}}$$
es el coeficiente de similaridad de Gower,

- $p_1$ es el ńumero de variables cuantitativas continuas,
- $p_2$ es el ńumero de variables binarias,
- $p_3$ es el ńumero de variables cualitativas(no binarias),
- $a$ es el ńumero de coincidencias (1, 1) en las variables binarias,
- $d$ es el número de coincidencias (0, 0) en las variables binarias,
- $\alpha$ es el ńumero de coincidencias en las variables cualitativas (no binarias) y
- $G_h$ es el rango (o recorrido) de la $h$-ésima variable cuantitativa.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Objetivo de la técnica</a>

En este apartado de análisis cluster nos vamos a centrar en dos objetivos. Comenzaremos haciendo uso de todas las variables, exceptuando la variable *y*, con el objetivo de analizar si es viable la agrupación en clusters de los datos y analizar la estructura interna del conjunto de datos. Para ello, se utilizará la distancia de *Gower*, para poder hacer uso de las variables categóricas.

Finalmente, se tomarán las componentes principales calculadas en el apartado correspondiente a *PCA* y se analizará la posibilidad de generar agrupaciones haciendo uso solo de las variables numéricas y tomando solo la información más relevante, es decir, la información que proporcionan las componentes principales. En este análisis, se llevará a cabo un estudio de los clusters generados con el objetivo de identificar algún perfil de interés para campañas futuras.

Cabe mencionar que la selección de objetivos y variables a utilizar ha sido en base a las intuiciones desarrolladas en *Breves Inntuiciones* y al análisis realizado en *Análisis de correlaciones*.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Procedimiento a seguir en el análisis</a>

Primeramente, analizaremos el estadístico de **Hopkins**, que permite evaluar la tendencia de clustering de un dataset mediante el cálculo de la probabilidad de que dichos datos procedan de una distribución uniforme. Por ello, somos capaces de evaluar si tiene sentido hacer un análisis cluster de nuestros datos si obtenemos valores alejados a 0.5 en el estadístico.

Acto seguido, realizaremos un análisis **VAT** con el objetivo de evaluar visualmente si los datos muestran indicios de algún tipo de agrupación.

Luego, se utilizará el criterio del codo, con el objetivo de ver cual es el número de clusters necesarios para realizar una correcta agrupación. A continuación, se utilizarán técnicas de clustering jerárquico o no jerárquico para crear las agrupaciones dependiendo de las características del problema al que nos enfrentemos. 

Por último, una vez realizado el clustering se evaluará la calidad de la agrupación con **Silhouette** y se tratarán de alcanzar los objetivos marcados.
<br/>

#### 1.Objetivo {.tabset .tabset-pills}

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Preparación de datos a analizar</a>

Comenzamos tratando el primer objetivo marcado, para ello, tomamos el conjunto de datos y seleccionamos todas las variables a excepción de *emp.var.ra*, *nr.employed* (variables definidas redundantes en el análisis de correlaciones) y *duration* (variable que no permite una predicción real).

```{r}
Objective1<-sinOutliers%>%select(-duration,-emp.var.rate,-nr.employed)
```

Por otro lado, en este apartado se utilizará la distancia de *Gower*, ya que nos permite hacer uso de todas las variables del dataset sin ningún tipo de transformación. Antes de realizar el estudio tomaremos una muestra de un tamaño reducido, dada la gran cantidad de datos que tenemos y el esfuerzo computacional que conlleva calcular todas las distancias, por ejemplo.

```{r}
set.seed(1234)
#muestra de tamaño menor, donde se mantiene la proporcion de la clase positiva y negativa
conjuntoNo <- Objective1 %>% filter(y=="no") 
conjuntoYes <- Objective1 %>% filter(y=="yes") 
n = 1500

muestraNo<- conjuntoNo %>% sample_n(size=round(0.8873458*n, 0), replace = FALSE) 
muestraYes<- conjuntoYes %>% sample_n(size=round(0.1126542*n, 0), replace = TRUE)

Objective1 <- rbind(muestraNo, muestraYes)
```

Por último, eliminamos la variable *y* de nuestros conjuntos de datos, ya que agrupa las observaciones en dos grupos.

```{r}
objective1<-Objective1%>%select(-y)
```

<br />

**Clustering**

Comenzamos calculando las distancias de *Gower* de nuestros datos para la realización de la estrategia definida.

```{r}
gower<-sapply(1:nrow(objective1), function(i) gower_dist(objective1[i,],objective1))
gowerDist <- as.dist(gower)
gowerMatrix <- as.matrix(gower)
```

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Hopkins</a>

Tal y como hemos tratado anteriormente, calculamos el estadístico de *Hopkins* para poder intuir la existencia de una agrupación en clusters de los datos. Para ello hacemos uso de las variables numéricas, ya que no podemos hacer uso de las variables categóricas para calcular el estadístico de *Hopkins* y **R** no permite introducir nuestras propias distancias (no podemos hacer uso de *Gower*).

```{r echo=FALSE}
set.seed(1234)
h<-hopkins(objective1%>%select_if(is.numeric))
print(paste("Estadístico de Hopkins: ",h))
```

Observamos que el estadístico de *Hopkins* tiene un valor alejado a $0.5$ (*Hopkins*=0.99), por lo tanto, el conjunto de datos no sigue una distribución espacial uniforme y su estructura podría contener algún tipo de agrupación.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">VAT</a>

Realizamos el análisis visual de la tendencia del cluster aplicando la distancia de *Gower* al conjunto de datos que mantiene las variables categóricas sin transformar, además , volvemos a mencionar que hacemos uso de una muestra de tamaño reducido a causa del esfuerzo computacional que supondría calcular todas las distancias de todo el conjunto. 

```{r echo=FALSE, fig.align="center", echo=FALSE}
fviz_dist(dist.obj = gowerDist, show_labels = FALSE) + labs(title = "Distancias") + 
    theme(legend.position = "bottom") 
```

Finalmente, se observan claros patrones de bloques en el *VAT* y por ello consideramos que realizar un análisis cluster con nuestro conjunto de datos es coherente.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Criterio del codo</a>

Ahora, vamos a seleccionar el número óptimo de clusters en los que vamos a intentar agrupar los datos en base al *criterio del codo*. Para ello, vamos a hacer uso de la matriz con las distancias de *Gower*.

```{r fig.align="center",echo=FALSE, warning=FALSE}
calcular_totwithinss <- function(n_clusters, datos, iter.max = 1000, nstart = 50) {
    cluster_kmeans <- kmeans(centers = n_clusters, x = datos , iter.max = iter.max,
        nstart = nstart)
    
    return(cluster_kmeans$tot.withinss)
}

total_withinss <- map_dbl(.x = 1:10, .f = calcular_totwithinss, datos = gowerMatrix)

data.frame(n_clusters = 1:10, suma_cuadrados_internos = total_withinss) %>%
    ggplot(aes(x = n_clusters, y = suma_cuadrados_internos)) + geom_line(color = "steelblue")+
    geom_vline(xintercept = 4, linetype = 2) + geom_point() +
    scale_x_continuous(breaks = 1:10) + labs(title = "Evolución de la suma total de cuadrados intra-cluster") +
    theme_bw()
```

Podemos observar que la gráfica no proporciona un codo que resalte entre las demás opciones, sin embargo, consideramos que 4 clusters puede ofrecer una agrupación correcta de los datos, dado que, la suma de cuadrados internos es suficientemente baja y la diferencia entre generar 3, 4 o 5 clusters no es muy significativa. Por consiguiente, una vez realizamos el clustering puede ser interesante realizar un breve análisis de sensibilidad en base al número de clusters, evaluando la calidad de las clasificaciones con *Silhouette*.

##### CLUSTERING NO JERÁRQUICOS

Comenzamos haciendo uso de un clustering partitivo y dada la inmensa cantidad de datos que tenemos, el único algoritmo cluster que nos ofrece soluciones con un coste computacional razonable es *Clara*, técnica que llevaremos a cabo para agrupar los datos en 4 clusters.

```{r  fig.align="center", echo=FALSE}
set.seed(1234)
clara_clusters_end <- clara(x = gowerMatrix, k = 4, stand = TRUE, samples = 50,
                        pamLike = TRUE)

fviz_cluster(object = clara_clusters_end, ellipse.type = "t", geom = "point", pointsize = 1,alpha=0.3) +
  theme_bw() + labs(title = "Resultados clustering CLARA") + theme(legend.position = "bottom")
```

Una vez realizado el clustering, graficamos los datos con sus respectivos clusters en las dos dimensiones que más información recogen. Observamos que los datos aparentemente se agrupan correctamente en 4 clusters. A pesar de que se observen datos clasificados por un cluster dentro de otro cluster, solo estamos graficando los resultados en dos dimensiones y esto podría generar confusión.

Por otro lado, entre las dos dimensiones que más información recogen, sólo se explica aproximadamente el 50% de la estructura. Por ello, es necesario analizar la calidad del clustering con otro criterio como el *Silhouette*, ya que la visualización en dos dimensiones no aporta gran información.


<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Silhouette</a>
  
A continuación, vamos a evaluar la calidad del clustering realizado a través de *Silhouette*.

```{r  fig.align="center", echo=FALSE}
fviz_silhouette(sil.obj = clara_clusters_end, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic())
```

Observamos que la calidad de la agrupación es un poco escasa, sin embargo, en el cluster 4 se obtiene un valor *Silhouette* acorde a una buena agrupación de datos. En el resto de casos, su valor *Silhouette* es cercano a 0.15, un valor que podría indicar que algunos datos de esta agrupación están en un punto intermedio entre su cluster y otro. No obstante, no estamos ante unos valores preocupantes para considerar que se está dando un clustering erróneo.

Con el objetivo de obtener la mejor agrupación interna de los datos, realizamos un análisis de sensibilidad donde se evaluarán otras cantidades de clusters a agrupar.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Análisis de sensibilidad del número de clusters</a>

Los resultados mostrados a continuación corresponden al uso de 3, 5 y 6 clusters respectivamente.

```{r  fig.align="center", echo=FALSE}
set.seed(1234)
clara_clusters <- clara(gowerMatrix, k = 3,  stand = TRUE, samples = 50,
                        pamLike = TRUE)
p1<-fviz_silhouette(sil.obj = clara_clusters, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic()) + labs(title = "Nº clusters 3")

clara_clusters <- clara(gowerMatrix, k = 5,  stand = TRUE, samples = 50,
                        pamLike = TRUE)
p2<-fviz_silhouette(sil.obj = clara_clusters, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic()) + labs(title = "Nº clusters 5")

clara_clusters <- clara(gowerMatrix, k = 6,  stand = TRUE, samples = 50,
                        pamLike = TRUE)
p3<-fviz_silhouette(sil.obj = clara_clusters, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic()) + labs(title = "Nº clusters 6")

(p1 | p2) / p3
```

Tras probar con distintos números de clusters, observamos que ninguna de las opciones nos ofrece una mejora de los resultados. Es cierto, que agrupando los datos en 3 clusters obtenemos dos agrupaciones que presentan un valor de *silhouette* alejado de cero. Sin embargo, el tercer cluster da un valor tan bajo que hace pensar que nuestra idea de agrupar en 4 cluster es más correcta. Por consiguiente, consideramos que la agrupación en 4 clusters ya nos ofrece unos buenos resultados y no hacemos un uso excesivo de los datos, por ello, seleccionamos esta opción como la resultante del clustering partitivo.

*** 

##### CLUSTERING JERÁRQUICO

En este caso, vamos a realizar el estuido partiendo de un clustering jerárquico ¿aglomerativo?, donde daremos la posibilidad de aplicar todos los tipos de linkage. No obstante, las conclusiones que se obtendran seran a raíz del linkage *Ward* donde minimizaremos la la suma total de varianza intra-cluster. Con el objetivo de recordar el linkage *Ward*, a continuación mostramos su definición:

>- **Ward**: Se trata de un método general. La selección del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor óptimo de una función objetivo, pudiendo ser esta última cualquier función definida por el analista. El conocido método Ward’s minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster. En cada paso, se identifican aquellos 2 clusters cuya fusión conlleva menor incremento de la varianza total intra-cluster. 

Mencionar que cortaremos el dendograma en cuatro ramas gracias a la información obtenida en el criterio del codo.

```{r echo=FALSE}
inputPanel(
  selectInput(inputId="linkage", label = h6("Selección Linkage"),
              choices = list("single"="single","complete" = "complete", "average" = "average", "centroid" = "centroid","Ward min(Var)"="ward.D2"),
              selected = "ward.D2",  multiple = FALSE, selectize = TRUE)
)
```

```{r echo=FALSE}
renderPlot({
      set.seed(1234)
      hc <- hclust(d = gowerDist, method = input$linkage)
      set.seed(1234)
      fviz_dend(x = hc, k =4, cex = 0.6,show_labels = FALSE, rect = TRUE, k_colors =c(paletaContinua[1], paletaContinua[2], paletaContinua[5],paletaContinua[6]), rect_border = c(paletaContinua[1], paletaContinua[2], paletaContinua[5],paletaContinua[6]), rect_fill = TRUE) + labs(title = "Herarchical clustering", subtitle = "Distancia Gower, Lincage average, K=4")
}, height=7*75)
```

Observamos que logramos un dendograma donde el corte en cuanto ramas podría ser el más relevante. Esto se debe a que, si nos enfocamos en los clusters 1 y 2, el siguiente paso del dendrograma es unirlos. No obstante, la diferencia que hay entre los grupos es suficientemente alta como para considerar que esos datos pertenecen a grupos distintos. Lo mismo ocurre a la hora de analizar los clusters 3 y 4, y finalmente la diferencia de los clusters 1-2 con 3-4 es significativamente desigual. 

No obstante, vamos a analizar la calidad de la clasificación. Además, tal y como hemos hecho en el clustering partitivo, realizaremos un análisis de sensibilidad aplicando 3, 5 y 6 clusters.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Silhouette</a>

```{r  fig.align="center", echo=FALSE}
set.seed(1234)
hc_corte_end <- hcut(x = gowerDist, hc_func = "hclust", hc_method = "ward.D2", k = 4)
fviz_silhouette(sil.obj = hc_corte_end, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic())
```

Observamos que el clustering jerárquico en 4 agrupaciones no presenta una calidad excesivamente alta, los pesos *Silhouette*. Los cálculos son distintos de cero pero no se logra tener una gran diferencia. No obstante, consideramos que tampoco se obtiene una clasificación inadmisible dada las diferencias observadas en el dendograma y los valores *Silhouette* que están entorno a $0.2$.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Análisis de sensibilidad</a>

```{r echo=FALSE}
set.seed(1234)
hc_corte <- hcut(x = gowerDist, hc_func = "hclust", hc_method = "ward.D2", k = 3)
p1<-fviz_silhouette(sil.obj = hc_corte, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic()) + labs(title = "Nº clusters 3")

hc_corte <- hcut(x = gowerDist, hc_func = "hclust", hc_method = "ward.D2", k = 5)
p2<-fviz_silhouette(sil.obj = hc_corte, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic()) + labs(title = "Nº clusters 5")

hc_corte <- hcut(x = gowerDist, hc_func = "hclust", hc_method = "ward.D2", k = 6)
p3<-fviz_silhouette(sil.obj = hc_corte, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic()) + labs(title = "Nº clusters 6")

(p1 | p3) / (p2)
```

Observamos que obtenemos resultados muy parecidos en términos de calidad de clustering, por ello, consideramos que la mejor opción es la que hemos graficado, ya que todas las alternativas proporcionan un *Silhouette* medio similar y de esta manera podemos analizar la robustez de los resultados comparándolos con el clustering partitivo.

####  {-} 

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">

##### Conclusiones primer objetivo

</a>

Finalmente, para generar las conclusiones de este primer objetivo, tomamos los dos resultados finales logrados en el clustering partitivo y jerárquico. Para ello, incluimos en la muestra *objectivo1* la variable correspondiente a la clasificación cluster.

```{r}
objective1_CP<-Objective1%>%mutate(cluster=clara_clusters_end$cluster)
objective1_CJ<-Objective1%>%mutate(cluster=hc_corte_end$cluster)
```

Comenzamos, observando el número de usuarios que hay de la muestra en cada cluster en base al resultado de la llamada telefónica.

```{r echo=FALSE, fig.align="center",echo=FALSE, warning=FALSE}
counts<-objective1_CP%>%group_by(cluster,y)%>%count()

Barplot1<-ggplot(counts, aes(fill=y, y=n, x=cluster, label = n)) + 
    geom_bar(position="stack", stat="identity")+ylab("Counts")+
  scale_fill_manual(values=c("#BFEFFF", "#FFC0CB"))+
  geom_text(size = 3, position = position_stack(vjust = 0.5))+ labs(title = "Partitivo")

counts<-objective1_CJ%>%group_by(cluster,y)%>%count()
Barplot2<-ggplot(counts, aes(fill=y, y=n, x=cluster, label = n)) + 
    geom_bar(position="stack", stat="identity")+ylab("Counts")+
  scale_fill_manual(values=c("#BFEFFF", "#FFC0CB"))+
  geom_text(size = 3, position = position_stack(vjust = 0.5))+ labs(title = "Jerarquico")

Barplot1+Barplot2
```

Analizando las dos gráficas, se observa claramente que en los clusters de ambos casos clasifican los datos del resultado de llamadas de una manera muy similar, lo que nos informa de que los algoritmos tienen robustez al llegar a resultados parecidos. En conclusión, queda probado que dentro de la estructura de los datos se da la existencia de agrupaciones en clusters, logrando así el primer objetivo marcado.

<br/>

####  2.Objetivo

En este segundo objetivo, se analizará el conjunto de datos al completo y una muestra reducida de esta. Luego, siguiendo las conclusiones obtenidas en el análisis *PCA*, transformamos los dos conjuntos de datos con las cinco componentes principales.

```{r}
set.seed(1234)
Objective2<-sinOutliers
Objective2_less<-muestraFinal

PCA<-prcomp(Objective2%>% select_if(is.numeric), center=FALSE, scale.=FALSE, rank. = 5)

Objective2_PCA<-data.frame(PCA$x)
Objective2_PCA_less<-data.frame(as.matrix(Objective2_less%>% select_if(is.numeric))%*%as.matrix(PCA$rotation))
```

<br />

**Clustering**

A continuación, antes de realizar el clustering, realizamos el procedimiento definido, tal y como hemos llevado a cabo en el primer objetivo.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Hopkins</a>

```{r echo=FALSE}
set.seed(1234)
h<-hopkins(Objective2_PCA_less)
print(paste("Estadístico de Hopkins: ",h))
```

En este caso, también volvemos a obtener un estadístico de *Hopkins* alejado a $0.5$, por lo tanto, podemos intuir que la estructura de los datos transformados siguen una algún tipo de agrupación.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">VAT</a>

A continuación, se realiza el análisis visual de tendencia cluster. Se da la posibilidad de tomar diversas distancias para datos numéricos. No obstante, las conclusiones se tomarán en base a la distancia *Manhatam*.

```{r echo=FALSE}
inputPanel(
  selectInput(inputId="dist_tipo2", label = h6("Selección tipo de distancia"), choices = list("euclidean"="euclidean"
                                                                                              ,"maximum" = "maximum", "manhattan" = "manhattan", "canberra" = "canberra","minkowski"="minkowski"),
              selected = "manhattan",  multiple = FALSE, selectize = TRUE)
)
```

```{r echo=FALSE}
renderPlot({
  
  distancias <- dist(Objective2_PCA_less, method = input$dist_tipo2)
  
  fviz_dist(dist.obj = distancias, show_labels = FALSE) + labs(title = "Distancias") + 
    theme(legend.position = "bottom")
  
}, height=7*75)
```

Se vuelven a observar patrones de bloques en el VAT. Por lo tanto, realizamos el análisis cluster con nuestro conjunto de datos, ya que los resultados de *Hopkins* y *VAT* nos indican que es correcto.

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Criterio del codo</a>

```{r}
set.seed(1234)
fviz_nbclust(x = Objective2_PCA_less, FUNcluster = kmeans, method = "wss", k.max = 10, diss = get_dist(Objective2_PCA_less,method = "manhattan"), nstart = 50) +
  geom_vline(xintercept = 4, linetype = 2)
```

En este caso, volvemos a no tener un codo relevante en la gráfica. Podemos intuir que el codo podría encontrarse entre los valores 3, 4 o 5, luego, seleccionamos 4 como el número de clusters a agrupar debido a que la suma de cuadrados internos es suficientemente baja y entre agrupar en 4 o 5 clusters no hay gran diferencia.

<br/>

##### Clustering partitivo - CLARA

A continuación, realizamos el clustering a través del metodo *CLARA* y visualizamos los resultados en las dos dimensiones que mayor información recogen en el clustering.

```{r fig.align="center", echo = FALSE}
set.seed(1234)
clara_clusters_end <- clara(x = Objective2_PCA_less, k = 4, metric = "manhattan", stand = TRUE, samples = 50,
    pamLike = TRUE)

fviz_cluster(object = clara_clusters_end, ellipse.type = "t", geom = "point", pointsize = 1,alpha=0.3) +
  theme_bw() + labs(title = "Resultados clustering CLARA") + theme(legend.position = "bottom")
```

Se observa que en los clusters 1, 3 y 4, hay varios datos que aparentemente están en más de un cluster, en el cluster al que no pertenecen o fuera de las elipses. Tal y como hemos comentado en el anterior caso, esto se puede deber al hecho de proyectar los resultados en solo dos dimensiones. 

Por otro lado, también se debe comentar que la calidad del segundo cluster probablemente sea aceptable, dado que proyectando únicamente en dos dimensiones ya podemos observar que generalmente las observaciones pertenecientes a ese clusters están alejados del resto de clusters y además en ese cluster no se observan gran cantidad de individuos que pertenezcan a otra agrupación. Por ello, el análisis de los individuos del cluster 2 puede ser relevante a la hora de identificar algún tipo de perfil.

```{r echo=FALSE}
Objective2_PCA_less<-Objective2_PCA_less%>%mutate(cluster=clara_clusters_end$cluster)
Objective2_PCA_less$cluster<-as.factor(Objective2_PCA_less$cluster)

Objective2_less<-Objective2_less%>%mutate(cluster=clara_clusters_end$cluster)
Objective2_less$cluster<-as.factor(Objective2_less$cluster)
df<-Objective2_less%>%group_by(y,cluster)%>%count()
```

Adicionalmente, proyectamos los datos clasificados en base a las 3 componentes principales más relevantes y haciendo uso de diversos ángulos podemos llegar a observar una visualización espacial de los clusters correcta.

```{r echo=FALSE, fig.align="center",echo=FALSE, warning=FALSE}
p <- plot_ly(Objective2_PCA_less, x=~PC1, y=~PC2, 
z=~PC3, color=~cluster) %>%
     add_markers(size=1.5)
p
```

<a style="text-decoration: 
none; border-bottom: 
1px solid #add8e6; 
color: #add8e6;">Silhouette</a>

```{r echo=FALSE, fig.align="center"}
fviz_silhouette(sil.obj = clara_clusters_end, print.summary = TRUE, palette = paletaContinua, ggtheme = theme_classic())
```

A la hora de evaluar la calidad del clustering, se observa que logramos una mejora en la clasificación de los datos haciendo uso únicamente de las 5 componentes principales respecto al caso anterior donde hemos utilizado todas las variables con la distancia de *Gower*. Por lo tanto, podemos concluir que haciendo uso de menos cantidad de información pero más relevante se puede obtener un mejor resultado.

Por otro lado, se observa que todos los clusters presentan un valor de *Silhouette* aceptable, es decir, alejado de cero. Además, se corrobora la idea anterior de que la calidad de la segunda agrupación es alta ya que su valor de *Silhouette* supera el $0.5$. Finalmente, podemos concluir que la calidad del clustering realizado es correcta.

##### Conclusiones

Con el objetivo de lograr algunas conclusiones relevantes para campañas futuras vamos a analizar en profundidad algunos de los clusters generados. Para ello, haremos uso de diversas gráficas y analizaremos los mediodes que nos proporciona el algoritmo de clasificación que se ha llevado a cabo.

**Clasificación de los clusters en función de la variable y**

```{r echo=FALSE, fig.align="center",echo=FALSE, warning=FALSE}
fig <- df %>% plot_ly(x = ~cluster, y = ~n, color = ~y, type = 'bar') 
fig
```

Gracias a esta gráfica podemos observar que en el tercer cluster no se clasifica ningún usuario que acepte la llamada. Por otro lado, en la cuarta agrupación observamos que la proporción de llamadas aceptadas o rechazadas es similar y por lo tanto podría darse el caso que en ese tipo  de individuos haya una mayor probabilidad de que se acepte la llamada. En consiguiente, consideramos que de cara a futuras campañas sería interesante analizar en profundidad los individuos del tercer cluster para evitar el fracaso y los del cuarto cluster para potenciar el éxito.

**Análisis de perfiles**

```{r echo=FALSE}
medioide1<-clara_clusters_end$medoids[1,]
index<-which(medioide1[1]==Objective2_PCA_less[,1] & medioide1[2]==Objective2_PCA_less[,2] & medioide1[3]==Objective2_PCA_less[,3] & medioide1[4]==Objective2_PCA_less[,4] & medioide1[5]==Objective2_PCA_less[,5] )
medioide1<-Objective2_less[index,]

medioide2<-clara_clusters_end$medoids[2,]
index<-which(medioide2[1]==Objective2_PCA_less[,1] & medioide2[2]==Objective2_PCA_less[,2] & medioide2[3]==Objective2_PCA_less[,3] & medioide2[4]==Objective2_PCA_less[,4] & medioide2[5]==Objective2_PCA_less[,5] )
medioide2<-Objective2_less[index,]

medioide3<-clara_clusters_end$medoids[3,]
index<-which(medioide3[1]==Objective2_PCA_less[,1] & medioide3[2]==Objective2_PCA_less[,2] & medioide3[3]==Objective2_PCA_less[,3] & medioide3[4]==Objective2_PCA_less[,4] & medioide3[5]==Objective2_PCA_less[,5] )
medioide3<-Objective2_less[index,]

medioide4<-clara_clusters_end$medoids[4,]
index<-which(medioide4[1]==Objective2_PCA_less[,1] & medioide4[2]==Objective2_PCA_less[,2] & medioide4[3]==Objective2_PCA_less[,3] & medioide4[4]==Objective2_PCA_less[,4] & medioide4[5]==Objective2_PCA_less[,5] )
medioide4<-Objective2_less[index,]

medioides<-rbind(medioide1,medioide2,medioide3,medioide4)

medioides%>% head(4) %>% kable() %>% kableExtra::kable_styling(full_width = F) %>%
  row_spec(row = 0,  background = "lightblue") %>%
  column_spec(column = 23, width = "3cm", background = "#ecfefe") %>%
  scroll_box(width = "900px", height = "250")
```

<br/>

Gracias a los mediodes podemos observar que los individuos del tercer cluster tienen una edad entorno a 42 años, tienen llamadas de duración relativamente corta (algo que concuerda con lo dicho en las *Breves Intuiciones*). No obstante, esta información no consideramos que pueda llegar a ser del todo relevante ya que obtenemos unos mediodes con características muy similares y no se observa ninguna caracterización, por ello, vamos a generar una gráfica en la que podamos observar el número de individuos que hay en cada categoría de nuestros factores en base a la variable y al cluster al que pertenecen. Por otro lado, visualizamos también las variables numéricas para ver la distribución de los datos dentro de estas agrupaciones.

```{r echo=FALSE}
inputPanel(
  shiny::selectInput("cluster",label="Selección de cluster", choices = c(1,2,3,4)),
  shiny::selectInput("cluster_factor",label="Selección de variable", choices = Objective2_less%>%select_if(negate(is.numeric))%>%select(-y,-cluster)%>%colnames(),selected = "marital"),
  shiny::selectInput("numerica", label="Selección de variable", choices = Objective2_less%>%select_if(is.numeric)%>%colnames())
)
```

```{r echo=FALSE}
renderPlot({
  counts<-data.frame(Objective2_less%>%filter(cluster==input$cluster)%>%group_by_(input$cluster_factor)%>%count(y))
  
  colnames(counts)=c("var","y","Counts")

  columnas<-c(pull(counts%>%select(var)%>% distinct()))
  
  no=pull(counts%>%filter(y=="no"))
  yes=pull(counts%>%filter(y=="yes"))
  
  #la variable job puede llegar a dar problemas
  if((length(columnas)!=2) & (length(no)==length(yes)) & (input$cluster_factor!="job")){
    
    mAx<-max(c(max(no),max(yes)))
    df<-rbind(no,yes)
    df<-as.data.frame(df)
    colnames(df)=c(columnas)
    df <- rbind(rep(mAx,length(no)) , rep(0,length(no)) , df)
    df<-as.data.frame(df)
    library(RColorBrewer)
    coul <- brewer.pal(3, "BuPu")
    #colors_border <- coul
    colors_border <- paletaContinua
    library(scales)
    colors_in <- alpha(coul,0.3)
    
    radarchart( df  , axistype=1 , 
                #custom polygon
                pcol=colors_border , pfcol=colors_in , plwd=4 , plty=1,
                #custom the grid
                cglcol="grey", cglty=1, axislabcol="grey", caxislabels=seq(0,41188,5), cglwd=0.8,
                #custom labels
                vlcex=0.8 
    )
    legend(x=0.7, y=1, legend = rownames(df[-c(1,2),]), bty = "n", pch=20 , col=colors_in , text.col = "grey", cex=1.2, pt.cex=3)
  }else{
    
    ggplot(counts, aes(x=var,y=Counts, fill=y)) +
      geom_bar(position="dodge", stat="identity")+
      geom_text(aes(label=Counts), position=position_dodge(width=0.9), vjust=-0.25,angle = 270,size=3)+
      coord_flip() +
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  }

}, height=7*75)
```

<br/>

*Ejemplo de análisis de pérfiles con el segundo cluster*

A continuación, vamos a analizar los individuos que más se clasifican en el cuarto cluster con el objetivo de obtener qué tipo de usuarios tienden a tener una mayor probabilidad de aceptar la llamada. Los individuos de este cluster que aceptan la llamada se tratan de trabajadores de clase baja (*blue-collar*), en la administración o en el sector técnico. Son individuos que están casados y con una mayor frecuencia tienden a tener un título universitario. En el resto de variables categóricas no observamos una caracterización diferencial de gran relevancia.

<br/>

```{r echo=FALSE}
renderPlot({
  datosDF<-data.frame(var=pull(Objective2_less%>%filter(cluster==input$cluster)%>%select(input$numerica)),y=pull(Objective2_less%>%filter(cluster==input$cluster)%>%select(y)))
  
p1 <- ggplot(data=datosDF, aes(x="", y=var)) + 
      geom_violin(color="black", fill="#a5bfde") +
      xlab("class") +
      theme(legend.position="none") +
      xlab("")+ylab(input$numerica) + 
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p2 <- ggplot(data=datosDF, aes(x=var, fill=y)) +
      geom_density(alpha=0.3)+xlab(input$numerica) + 
      scale_fill_brewer(palette="Set3")+
      theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p <-  ggplot(datosDF, aes(x=var, y=y)) +
        scale_fill_brewer(palette="Set3")+
        geom_point() +
        geom_line(color='blue')+
        theme(legend.position="none")+xlab(input$numerica) +
        theme(plot.background = element_rect(fill = "#FFFFFF",
                            colour = NA), panel.background = element_rect(fill = "#FFFFFF", colour = NA))
  
  p3 <- ggMarginal(p, margins = 'x', color="blue", size=10)
  
        

  (p1 | p3) / p2
}, height=7*75)
```

Por otro lado, se trata de individuos en torno a los 30 - 40 años generalmente. Clientes con los que se ha contactado generalmente hasta en 5 ocasiones en esa misma campaña y que no habían sido contactados anteriormente en otras. Por último, se trata de individuos que se contactaron cuando la tasa euribor se encontraba en torno a los 5 o 4 puntos.

<br/>

# CONCLUSIONES FINALES

El objetivo principal de este estudio es recoger las características principales que permiten predecir el éxito o fracaso de las llamadas de telemarketing. Se trata de un dataset real, correspondiente a un banco minorista portugués que pretendía vender depósitos bancarios a largo plazo. Es evidente que un estudio profundo de la información es indispensable, de tal manera que se pueda favorecer al éxito de las llamadas y, por tanto, al objetivo del banco. Analizar las características de las observaciones, variables, posibles agrupaciones de individuos, relaciones entre atributos etc. es fundamental para poder sacar conclusiones robustas y tomar decisiones de cara al futuro. 

Para ello, se han desarrollado diferentes análisis y técnicas que han permitido explicar las distintas relaciones entre variables, individuos, agrupaciones de estos... Comenzando por el **Análisis Univariante**, se logró estructurar una idea general del conjunto de datos con el que se iba a trabajar. Asimismo, se estudiarion las medidas estadísticas más importantes para comprender el tipo y rango de los atributos, entre otras características. Profundizando más en el estudio, se detectaron individuos outliers, de tal manera que se redujo el número de observaciones para evitar posibles alteraciones en análisis posteriores. Para terminar con esta sección, se realizaron unas primeras *Breves Intuiciones* que dan lugar a diferentes enfoques de estudio. 

En lo que al **Análisis Multivariante** respecta, está dividido en dos secciones. Por una parte, el estudio de las variables, las relaciones entre ellas y posibles combinaciones de atributos. Por otra, el estudio de los individuos, agrupaciones que pueden formar y perfiles de cliente que puedan sugerir información adicional a lo previamente mencionado.


## Conclusiones en torno a Variables

***

Para poder formalizar una idea global de las variables se aplican distintas técnicas: matriz de correlaciones, Análisis PCA, Factorial y de Correspondencias. Todas ellas nos conducen a conclusiones similares.

- Hay redundancia de variables. De hecho, los atributos *euribor3m*, *emp.var.rate.*, *nr.employed* e incluso *cons.price.idx.* se pueden explicar entre sí. La técnica PCA y la matriz de correlaciones nos ofrecen esta información. De todas ellas, hemos considerado *euribor3m* como la primordial. 

- Tras este primer conjunto de variables, *pdays* y *previous* adquieren importancia. Presentan también correlación positiva entre ellas, aunque no se explican entre sí. Se necesita la variabilidad de ambas para recoger toda la información.

- A pesar de que aparentemente *duration* ofrecía información relevante al estudio, concluimos que finalmente no era así, ya que no permite realizar una predicción real.

- En lo que a las variables numéricas refiere, el Análisis PCA sugiere la construcción de 5 Compononentes Principales que recogen en torno al 80% de la variabilidad de los atributos. El Análisis Factorial proporciona resultados similares.

- Las variables *job* y *marital* han demostrado ser importantes y tener categorías relacionadas. No obstante, consideramos que la información aportada por las variables cualitativas es menor que la proporcionada por las numéricas.


## Conclusiones en torno a Individuos

***

Al igual que en el *Análisis Univariante*, a través de distancias se ha intentado detectar individuos anómalos considerando el conjunto de variables completo, es decir, todas sus características. Se suprimen en torno a 5.000 observaciones que distorsionaban los resultados obtenidos.  

Una vez obtenido el conjunto final de individuos con los que se va a trabajar, nos disponemos a aplicar técnicas que permiten identificar posibles clusters.

- En primer lugar, se tratan de identificar agrupaciones de individuos considerando todas las variables del conjunto de datos, excepto *duration*, las consideradas redundantes y la variable de salida *y*. El resultado final que se obtiene son agrupaciones en 4 clusters. Esto permite clasificar a los individuos en uno de los cuatro grupos, algo realmente útil, puesto que al trabajar con tanta información una posible clasificación es fundamental. De esta manera, al contactar con otro cliente, y considerando las características de cada agrupación, se podrá clasificar el nuevo individuo y así preveer el resultado de la llamada. Todo ello permitirá crear perfiles de cliente.

- Un segundo enfoque es utilizar las Componentes Principales para detectar clusters. Repitiendo el proceso anterior, el resultado obtenido es más favorable. Por lo tanto, podemos concluir que haciendo uso de menos cantidad de información pero más relevante se puede obtener un mejor resultado. 


## Conclusiones en torno a Perfiles de Individuos

***

Finalmente, como cierre del estudio queremos mencionar un aspecto fundamental del mismo: la creación de perfiles de clientes. Un perfil de cliente conforma el conjunto de características (atributos) que describen el tipo de persona que es más/menos propensa a comprar el depósito bancario a largo plazo. Evidentemente, este es el propósito final del banco; poder lograr agrupar las características de los individuos más y menos propensos a comprar el servicio. En nuestro caso, hemos estudiado el ejemplo del cluster 2, en el que se analiza qué tipo de persona es más propensa a aceptar la llamada. Los resultados obtenidos son los siguientes: trabajadores de clase baja, en la administración o en el sector técnico, casados y con un título universitario. Esto sería un ejemplo de la caracterización de los clusters. 

En suma, la identificación de perfiles de clientes es la clave para el éxito de la campaña. El proceso mencionado anteriormente se podría repetir con el resto de agrupaciones, de tal manera que las campañas de marketing posteriores recopilaran toda esta información a su favor. De esta manera, se contactaría con aquellos clientes más potenciales, favoreciendo el éxito del proceso de marketing. 
